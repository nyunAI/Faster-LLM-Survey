2024-01-18 09:01:33 INFO [__main__] max_memory: {0: '40GIB'}
2024-01-18 09:01:33 INFO [__main__] loading model and tokenizer
2024-01-18 09:01:34 INFO [auto_gptq.modeling._base] lm_head not been quantized, will be ignored when make_quant.
2024-01-18 09:01:37 WARNING [auto_gptq.nn_modules.fused_llama_mlp] Skipping module injection for FusedLlamaMLPForQuantizedModel as currently not supported with use_triton=False.
2024-01-18 09:02:22 INFO [__main__] model and tokenizer loading time: 48.5091s
2024-01-18 09:02:22 INFO [__main__] model quantized: True
2024-01-18 09:02:22 INFO [__main__] quantize config: {'bits': 4, 'group_size': 128, 'damp_percent': 0.01, 'desc_act': False, 'static_groups': False, 'sym': True, 'true_sequential': True, 'model_name_or_path': 'exports/Llama-2-7b-hf/quant/gptq/q4_128g', 'model_file_base_name': 'gptq_model-4bit-128g'}
2024-01-18 09:02:22 INFO [__main__] model device map: OrderedDict([('', 0)])
2024-01-18 09:02:22 INFO [__main__] loading data
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Map: 100%|██████████| 10/10 [00:00<00:00, 595.60 examples/s]
2024-01-18 09:02:22 INFO [__main__] generation config: {'max_length': 20, 'max_new_tokens': 128, 'min_length': 0, 'min_new_tokens': 128, 'early_stopping': False, 'max_time': None, 'do_sample': False, 'num_beams': 4, 'num_beam_groups': 1, 'penalty_alpha': None, 'use_cache': True, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'epsilon_cutoff': 0.0, 'eta_cutoff': 0.0, 'diversity_penalty': 0.0, 'repetition_penalty': 1.0, 'encoder_repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'bad_words_ids': None, 'force_words_ids': None, 'renormalize_logits': False, 'constraints': None, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'forced_decoder_ids': None, 'sequence_bias': None, 'guidance_scale': None, 'low_memory': None, 'num_return_sequences': 4, 'output_attentions': False, 'output_hidden_states': False, 'output_scores': False, 'return_dict_in_generate': False, 'pad_token_id': 2, 'bos_token_id': None, 'eos_token_id': None, 'encoder_no_repeat_ngram_size': 0, 'decoder_start_token_id': None, 'num_assistant_tokens': 5, 'num_assistant_tokens_schedule': 'heuristic', 'generation_kwargs': {}, '_from_model_config': False, 'transformers_version': '4.36.2'}
2024-01-18 09:02:22 INFO [__main__] benchmark generation speed
  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:05<?, ?it/s, num_tokens=512, speed=90.8273tokens/s, time=5.64] 10%|█         | 1/10 [00:05<00:50,  5.65s/it, num_tokens=512, speed=90.8273tokens/s, time=5.64] 10%|█         | 1/10 [00:10<00:50,  5.65s/it, num_tokens=512, speed=114.8129tokens/s, time=4.46] 20%|██        | 2/10 [00:10<00:39,  4.96s/it, num_tokens=512, speed=114.8129tokens/s, time=4.46] 20%|██        | 2/10 [00:14<00:39,  4.96s/it, num_tokens=512, speed=123.8144tokens/s, time=4.14] 30%|███       | 3/10 [00:14<00:32,  4.59s/it, num_tokens=512, speed=123.8144tokens/s, time=4.14] 30%|███       | 3/10 [00:18<00:32,  4.59s/it, num_tokens=512, speed=123.1425tokens/s, time=4.16] 40%|████      | 4/10 [00:18<00:26,  4.42s/it, num_tokens=512, speed=123.1425tokens/s, time=4.16] 40%|████      | 4/10 [00:22<00:26,  4.42s/it, num_tokens=512, speed=117.5243tokens/s, time=4.36] 50%|█████     | 5/10 [00:22<00:22,  4.41s/it, num_tokens=512, speed=117.5243tokens/s, time=4.36] 50%|█████     | 5/10 [00:27<00:22,  4.41s/it, num_tokens=512, speed=121.0772tokens/s, time=4.23] 60%|██████    | 6/10 [00:27<00:17,  4.35s/it, num_tokens=512, speed=121.0772tokens/s, time=4.23] 60%|██████    | 6/10 [00:31<00:17,  4.35s/it, num_tokens=512, speed=118.1134tokens/s, time=4.33] 70%|███████   | 7/10 [00:31<00:13,  4.35s/it, num_tokens=512, speed=118.1134tokens/s, time=4.33] 70%|███████   | 7/10 [00:35<00:13,  4.35s/it, num_tokens=512, speed=118.0234tokens/s, time=4.34] 80%|████████  | 8/10 [00:35<00:08,  4.35s/it, num_tokens=512, speed=118.0234tokens/s, time=4.34] 80%|████████  | 8/10 [00:40<00:08,  4.35s/it, num_tokens=512, speed=113.1557tokens/s, time=4.52] 90%|█████████ | 9/10 [00:40<00:04,  4.41s/it, num_tokens=512, speed=113.1557tokens/s, time=4.52] 90%|█████████ | 9/10 [00:44<00:04,  4.41s/it, num_tokens=512, speed=123.1316tokens/s, time=4.16]100%|██████████| 10/10 [00:44<00:00,  4.34s/it, num_tokens=512, speed=123.1316tokens/s, time=4.16]100%|██████████| 10/10 [00:44<00:00,  4.45s/it, num_tokens=512, speed=123.1316tokens/s, time=4.16]
2024-01-18 09:03:07 INFO [__main__] generated 5120 tokens using 44.330591917037964 seconds, generation speed: 115.49586365960941 tokens/s
