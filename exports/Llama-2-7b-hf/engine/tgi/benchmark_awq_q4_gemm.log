[2m2024-01-20T14:41:58.461524Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Args { model_id: "/home/shwu/LLM-Efficiency-Survey/exports/Llama-2-7b-hf/quant/awq/q4_gemm", revision: None, validation_workers: 2, sharded: None, num_shard: None, quantize: Some(Awq), speculate: None, dtype: None, trust_remote_code: false, max_concurrent_requests: 128, max_best_of: 2, max_stop_sequences: 4, max_top_n_tokens: 5, max_input_length: 1024, max_total_tokens: 2048, waiting_served_ratio: 1.2, max_batch_prefill_tokens: 4096, max_batch_total_tokens: None, max_waiting_tokens: 20, hostname: "0.0.0.0", port: 3000, shard_uds_path: "/tmp/text-generation-server", master_addr: "localhost", master_port: 29500, huggingface_hub_cache: None, weights_cache_override: None, disable_custom_kernels: false, cuda_memory_fraction: 1.0, rope_scaling: None, rope_factor: None, json_output: false, otlp_endpoint: None, cors_allow_origin: [], watermark_gamma: None, watermark_delta: None, ngrok: false, ngrok_authtoken: None, ngrok_edge: None, env: false }
[2m2024-01-20T14:41:58.461751Z[0m [32m INFO[0m [1mdownload[0m: [2mtext_generation_launcher[0m[2m:[0m Starting download process.
[2m2024-01-20T14:42:00.987516Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Files are already present on the host. Skipping download.

[2m2024-01-20T14:42:01.465109Z[0m [32m INFO[0m [1mdownload[0m: [2mtext_generation_launcher[0m[2m:[0m Successfully downloaded weights.
[2m2024-01-20T14:42:01.465392Z[0m [32m INFO[0m [1mshard-manager[0m: [2mtext_generation_launcher[0m[2m:[0m Starting shard [2m[3mrank[0m[2m=[0m0[0m
[2m2024-01-20T14:42:05.316450Z[0m [33m WARN[0m [2mtext_generation_launcher[0m[2m:[0m Could not import Flash Attention enabled models: No module named 'vllm'

[2m2024-01-20T14:42:08.198265Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Server started at unix:///tmp/text-generation-server-0

[2m2024-01-20T14:42:08.272408Z[0m [32m INFO[0m [1mshard-manager[0m: [2mtext_generation_launcher[0m[2m:[0m Shard ready in 6.806248487s [2m[3mrank[0m[2m=[0m0[0m
[2m2024-01-20T14:42:08.370802Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Starting Webserver
[2m2024-01-20T14:42:08.446924Z[0m [33m WARN[0m [2mtext_generation_router[0m[2m:[0m [2mrouter/src/main.rs[0m[2m:[0m[2m246:[0m Could not find tokenizer config locally and no revision specified
[2m2024-01-20T14:42:08.446957Z[0m [33m WARN[0m [2mtext_generation_router[0m[2m:[0m [2mrouter/src/main.rs[0m[2m:[0m[2m258:[0m no pipeline tag found for model /home/shwu/LLM-Efficiency-Survey/exports/Llama-2-7b-hf/quant/awq/q4_gemm
[2m2024-01-20T14:42:08.452572Z[0m [32m INFO[0m [2mtext_generation_router[0m[2m:[0m [2mrouter/src/main.rs[0m[2m:[0m[2m277:[0m Warming up model
[2m2024-01-20T14:42:10.491320Z[0m [33m WARN[0m [2mtext_generation_router[0m[2m:[0m [2mrouter/src/main.rs[0m[2m:[0m[2m291:[0m Model does not support automatic max batch total tokens
[2m2024-01-20T14:42:10.491343Z[0m [32m INFO[0m [2mtext_generation_router[0m[2m:[0m [2mrouter/src/main.rs[0m[2m:[0m[2m313:[0m Setting max batch total tokens to 16000
[2m2024-01-20T14:42:10.491348Z[0m [32m INFO[0m [2mtext_generation_router[0m[2m:[0m [2mrouter/src/main.rs[0m[2m:[0m[2m314:[0m Connected
[2m2024-01-20T14:42:18.411728Z[0m [32m INFO[0m [1mgenerate[0m[1m{[0m[3mparameters[0m[2m=[0mGenerateParameters { best_of: None, temperature: None, repetition_penalty: None, top_k: None, top_p: None, typical_p: None, do_sample: false, max_new_tokens: Some(128), return_full_text: None, stop: [], truncate: None, watermark: false, details: false, decoder_input_details: false, seed: None, top_n_tokens: None } [3mtotal_time[0m[2m=[0m"4.247233881s" [3mvalidation_time[0m[2m=[0m"361.721Âµs" [3mqueue_time[0m[2m=[0m"85.609Âµs" [3minference_time[0m[2m=[0m"4.246786737s" [3mtime_per_token[0m[2m=[0m"33.178021ms" [3mseed[0m[2m=[0m"None"[1m}[0m[2m:[0m [2mtext_generation_router::server[0m[2m:[0m [2mrouter/src/server.rs[0m[2m:[0m[2m296:[0m Success
[2m2024-01-20T14:42:22.592716Z[0m [32m INFO[0m [1mgenerate[0m[1m{[0m[3mparameters[0m[2m=[0mGenerateParameters { best_of: None, temperature: None, repetition_penalty: None, top_k: None, top_p: None, typical_p: None, do_sample: false, max_new_tokens: Some(128), return_full_text: None, stop: [], truncate: None, watermark: false, details: false, decoder_input_details: false, seed: None, top_n_tokens: None } [3mtotal_time[0m[2m=[0m"4.178729193s" [3mvalidation_time[0m[2m=[0m"210.27Âµs" [3mqueue_time[0m[2m=[0m"43.778Âµs" [3minference_time[0m[2m=[0m"4.178475326s" [3mtime_per_token[0m[2m=[0m"32.644338ms" [3mseed[0m[2m=[0m"None"[1m}[0m[2m:[0m [2mtext_generation_router::server[0m[2m:[0m [2mrouter/src/server.rs[0m[2m:[0m[2m296:[0m Success
[2m2024-01-20T14:42:26.767253Z[0m [32m INFO[0m [1mgenerate[0m[1m{[0m[3mparameters[0m[2m=[0mGenerateParameters { best_of: None, temperature: None, repetition_penalty: None, top_k: None, top_p: None, typical_p: None, do_sample: false, max_new_tokens: Some(128), return_full_text: None, stop: [], truncate: None, watermark: false, details: false, decoder_input_details: false, seed: None, top_n_tokens: None } [3mtotal_time[0m[2m=[0m"4.172238711s" [3mvalidation_time[0m[2m=[0m"216.683Âµs" [3mqueue_time[0m[2m=[0m"36.285Âµs" [3minference_time[0m[2m=[0m"4.171985939s" [3mtime_per_token[0m[2m=[0m"32.59364ms" [3mseed[0m[2m=[0m"None"[1m}[0m[2m:[0m [2mtext_generation_router::server[0m[2m:[0m [2mrouter/src/server.rs[0m[2m:[0m[2m296:[0m Success
[2m2024-01-20T14:42:30.937991Z[0m [32m INFO[0m [1mgenerate[0m[1m{[0m[3mparameters[0m[2m=[0mGenerateParameters { best_of: None, temperature: None, repetition_penalty: None, top_k: None, top_p: None, typical_p: None, do_sample: false, max_new_tokens: Some(128), return_full_text: None, stop: [], truncate: None, watermark: false, details: false, decoder_input_details: false, seed: None, top_n_tokens: None } [3mtotal_time[0m[2m=[0m"4.168503202s" [3mvalidation_time[0m[2m=[0m"185.379Âµs" [3mqueue_time[0m[2m=[0m"44.301Âµs" [3minference_time[0m[2m=[0m"4.168273714s" [3mtime_per_token[0m[2m=[0m"32.564638ms" [3mseed[0m[2m=[0m"None"[1m}[0m[2m:[0m [2mtext_generation_router::server[0m[2m:[0m [2mrouter/src/server.rs[0m[2m:[0m[2m296:[0m Success
[2m2024-01-20T14:42:35.122781Z[0m [32m INFO[0m [1mgenerate[0m[1m{[0m[3mparameters[0m[2m=[0mGenerateParameters { best_of: None, temperature: None, repetition_penalty: None, top_k: None, top_p: None, typical_p: None, do_sample: false, max_new_tokens: Some(128), return_full_text: None, stop: [], truncate: None, watermark: false, details: false, decoder_input_details: false, seed: None, top_n_tokens: None } [3mtotal_time[0m[2m=[0m"4.182546817s" [3mvalidation_time[0m[2m=[0m"219.591Âµs" [3mqueue_time[0m[2m=[0m"46.086Âµs" [3minference_time[0m[2m=[0m"4.182281384s" [3mtime_per_token[0m[2m=[0m"32.674073ms" [3mseed[0m[2m=[0m"None"[1m}[0m[2m:[0m [2mtext_generation_router::server[0m[2m:[0m [2mrouter/src/server.rs[0m[2m:[0m[2m296:[0m Success
[2m2024-01-20T14:42:39.323906Z[0m [32m INFO[0m [1mgenerate[0m[1m{[0m[3mparameters[0m[2m=[0mGenerateParameters { best_of: None, temperature: None, repetition_penalty: None, top_k: None, top_p: None, typical_p: None, do_sample: false, max_new_tokens: Some(128), return_full_text: None, stop: [], truncate: None, watermark: false, details: false, decoder_input_details: false, seed: None, top_n_tokens: None } [3mtotal_time[0m[2m=[0m"4.198814852s" [3mvalidation_time[0m[2m=[0m"192.387Âµs" [3mqueue_time[0m[2m=[0m"47.661Âµs" [3minference_time[0m[2m=[0m"4.198575s" [3mtime_per_token[0m[2m=[0m"32.801367ms" [3mseed[0m[2m=[0m"None"[1m}[0m[2m:[0m [2mtext_generation_router::server[0m[2m:[0m [2mrouter/src/server.rs[0m[2m:[0m[2m296:[0m Success
[2m2024-01-20T14:42:43.500866Z[0m [32m INFO[0m [1mgenerate[0m[1m{[0m[3mparameters[0m[2m=[0mGenerateParameters { best_of: None, temperature: None, repetition_penalty: None, top_k: None, top_p: None, typical_p: None, do_sample: false, max_new_tokens: Some(128), return_full_text: None, stop: [], truncate: None, watermark: false, details: false, decoder_input_details: false, seed: None, top_n_tokens: None } [3mtotal_time[0m[2m=[0m"4.174637176s" [3mvalidation_time[0m[2m=[0m"204.494Âµs" [3mqueue_time[0m[2m=[0m"53.559Âµs" [3minference_time[0m[2m=[0m"4.174379329s" [3mtime_per_token[0m[2m=[0m"32.612338ms" [3mseed[0m[2m=[0m"None"[1m}[0m[2m:[0m [2mtext_generation_router::server[0m[2m:[0m [2mrouter/src/server.rs[0m[2m:[0m[2m296:[0m Success
[2m2024-01-20T14:42:47.686702Z[0m [32m INFO[0m [1mgenerate[0m[1m{[0m[3mparameters[0m[2m=[0mGenerateParameters { best_of: None, temperature: None, repetition_penalty: None, top_k: None, top_p: None, typical_p: None, do_sample: false, max_new_tokens: Some(128), return_full_text: None, stop: [], truncate: None, watermark: false, details: false, decoder_input_details: false, seed: None, top_n_tokens: None } [3mtotal_time[0m[2m=[0m"4.183553918s" [3mvalidation_time[0m[2m=[0m"175.082Âµs" [3mqueue_time[0m[2m=[0m"43.99Âµs" [3minference_time[0m[2m=[0m"4.183335109s" [3mtime_per_token[0m[2m=[0m"32.682305ms" [3mseed[0m[2m=[0m"None"[1m}[0m[2m:[0m [2mtext_generation_router::server[0m[2m:[0m [2mrouter/src/server.rs[0m[2m:[0m[2m296:[0m Success
[2m2024-01-20T14:42:51.842667Z[0m [32m INFO[0m [1mgenerate[0m[1m{[0m[3mparameters[0m[2m=[0mGenerateParameters { best_of: None, temperature: None, repetition_penalty: None, top_k: None, top_p: None, typical_p: None, do_sample: false, max_new_tokens: Some(128), return_full_text: None, stop: [], truncate: None, watermark: false, details: false, decoder_input_details: false, seed: None, top_n_tokens: None } [3mtotal_time[0m[2m=[0m"4.153675204s" [3mvalidation_time[0m[2m=[0m"199.154Âµs" [3mqueue_time[0m[2m=[0m"39.981Âµs" [3minference_time[0m[2m=[0m"4.153436263s" [3mtime_per_token[0m[2m=[0m"32.44872ms" [3mseed[0m[2m=[0m"None"[1m}[0m[2m:[0m [2mtext_generation_router::server[0m[2m:[0m [2mrouter/src/server.rs[0m[2m:[0m[2m296:[0m Success
[2m2024-01-20T14:42:56.039421Z[0m [32m INFO[0m [1mgenerate[0m[1m{[0m[3mparameters[0m[2m=[0mGenerateParameters { best_of: None, temperature: None, repetition_penalty: None, top_k: None, top_p: None, typical_p: None, do_sample: false, max_new_tokens: Some(128), return_full_text: None, stop: [], truncate: None, watermark: false, details: false, decoder_input_details: false, seed: None, top_n_tokens: None } [3mtotal_time[0m[2m=[0m"4.194392928s" [3mvalidation_time[0m[2m=[0m"171.778Âµs" [3mqueue_time[0m[2m=[0m"39.604Âµs" [3minference_time[0m[2m=[0m"4.194181714s" [3mtime_per_token[0m[2m=[0m"32.767044ms" [3mseed[0m[2m=[0m"None"[1m}[0m[2m:[0m [2mtext_generation_router::server[0m[2m:[0m [2mrouter/src/server.rs[0m[2m:[0m[2m296:[0m Success
[2m2024-01-20T14:42:56.044412Z[0m [32m INFO[0m [2mtext_generation_router::server[0m[2m:[0m [2mrouter/src/server.rs[0m[2m:[0m[2m974:[0m signal received, starting graceful shutdown
[2m2024-01-20T14:42:56.121861Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Terminating webserver
[2m2024-01-20T14:42:56.121986Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Waiting for webserver to gracefully shutdown
[2m2024-01-20T14:42:56.122008Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m webserver terminated
[2m2024-01-20T14:42:56.122012Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Shutting down shards
[2m2024-01-20T14:42:56.220248Z[0m [31mERROR[0m [1mshard-manager[0m: [2mtext_generation_launcher[0m[2m:[0m Shard complete standard error output:

You have loaded an AWQ model on CPU and have a CUDA device available, make sure to set your model on a GPU device in order to run your model. [2m[3mrank[0m[2m=[0m0[0m
[2m2024-01-20T14:42:56.220377Z[0m [31mERROR[0m [1mshard-manager[0m: [2mtext_generation_launcher[0m[2m:[0m Shard process was signaled to shutdown with signal 15 [2m[3mrank[0m[2m=[0m0[0m
