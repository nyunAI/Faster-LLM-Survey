[BENCHMARK] model_name llama_7b world_size 1 num_heads 32 num_kv_heads 32 num_layers 32 hidden_size 4096 vocab_size 32000 precision float16 batch_size 1 input_length 128 output_length 128 gpu_peak_mem(gb) 6.26 build_time(s) 0 tokens_per_sec 194.86 percentile95(ms) 658.375 percentile99(ms) 664.347 latency(ms) 656.895 compute_cap sm80
