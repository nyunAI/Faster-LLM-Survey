main: build = 1833 (326b418b)
main: built with cc (Debian 10.2.1-6) 10.2.1 20210110 for x86_64-linux-gnu
main: seed  = 1706106893
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from exports/Llama-2-7b-hf/engine/llamacpp/ggml-model-f16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) 
llm_load_print_meta: general.name     = LLaMA v2
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size       =    0.11 MiB
llm_load_tensors: using CUDA for GPU acceleration
llm_load_tensors: system memory used  =  250.11 MiB
llm_load_tensors: VRAM used           = 12603.02 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
....................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init: VRAM kv self = 256.00 MB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_build_graph: non-view tensors processed: 676/676
llama_new_context_with_model: compute buffer total size = 73.69 MiB
llama_new_context_with_model: VRAM scratch buffer: 70.50 MiB
llama_new_context_with_model: total VRAM used: 12929.52 MiB (model: 12603.02 MiB, context: 326.50 MiB)

system_info: n_threads = 32 / 12 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 1085.19 ms
perplexity: calculating perplexity over 655 chunks, batch_size=512
perplexity: 0.17 seconds per pass - ETA 1.83 minutes
[1]4.1747,[2]4.6894,[3]5.3386,[4]5.9095,[5]6.0409,[6]5.9587,[7]6.1314,[8]6.2198,[9]6.5411,[10]6.7223,[11]6.9383,[12]6.9865,[13]6.9114,[14]6.9886,[15]7.2108,[16]6.8711,[17]6.7548,[18]6.7445,[19]6.4249,[20]6.4203,[21]6.3459,[22]6.1771,[23]6.1471,[24]6.0570,[25]6.0441,[26]5.8882,[27]5.7059,[28]5.6069,[29]5.5250,[30]5.3745,[31]5.3368,[32]5.3564,[33]5.3127,[34]5.3413,[35]5.3573,[36]5.3818,[37]5.3765,[38]5.3751,[39]5.3890,[40]5.4387,[41]5.4603,[42]5.4977,[43]5.4596,[44]5.5135,[45]5.5230,[46]5.4999,[47]5.5239,[48]5.5033,[49]5.5039,[50]5.4714,[51]5.4716,[52]5.4616,[53]5.5088,[54]5.4962,[55]5.4819,[56]5.5115,[57]5.5303,[58]5.5582,[59]5.5793,[60]5.6285,[61]5.6251,[62]5.6864,[63]5.7212,[64]5.7279,[65]5.7702,[66]5.7812,[67]5.8003,[68]5.8204,[69]5.8552,[70]5.8928,[71]5.9196,[72]5.9540,[73]6.0058,[74]6.0159,[75]6.0269,[76]6.0444,[77]6.0594,[78]6.0488,[79]6.0752,[80]6.0743,[81]6.0894,[82]6.0949,[83]6.0462,[84]6.0360,[85]6.0322,[86]6.0152,[87]5.9584,[88]5.9349,[89]5.9204,[90]5.9131,[91]5.9332,[92]5.9325,[93]5.9342,[94]5.9342,[95]5.9635,[96]5.9620,[97]5.9542,[98]5.9486,[99]5.9375,[100]5.9398,[101]5.9620,[102]5.9575,[103]5.9744,[104]5.9834,[105]5.9851,[106]6.0018,[107]6.0043,[108]6.0190,[109]6.0178,[110]6.0144,[111]6.0340,[112]6.0534,[113]6.0559,[114]6.0564,[115]6.0651,[116]6.0537,[117]6.0581,[118]6.0829,[119]6.1034,[120]6.1368,[121]6.1531,[122]6.1751,[123]6.2154,[124]6.2326,[125]6.2241,[126]6.2597,[127]6.2944,[128]6.3196,[129]6.3037,[130]6.3129,[131]6.3069,[132]6.2980,[133]6.2838,[134]6.2915,[135]6.2890,[136]6.2776,[137]6.2716,[138]6.2547,[139]6.2477,[140]6.2444,[141]6.2193,[142]6.2148,[143]6.1872,[144]6.1681,[145]6.1591,[146]6.1490,[147]6.1558,[148]6.1571,[149]6.1502,[150]6.1502,[151]6.1564,[152]6.1497,[153]6.1365,[154]6.1291,[155]6.1364,[156]6.1341,[157]6.1490,[158]6.1516,[159]6.1535,[160]6.1561,[161]6.1688,[162]6.1414,[163]6.1300,[164]6.1054,[165]6.0748,[166]6.0472,[167]6.0099,[168]5.9793,[169]5.9653,[170]5.9540,[171]5.9292,[172]5.9154,[173]5.9002,[174]5.8721,[175]5.8514,[176]5.8379,[177]5.8182,[178]5.7969,[179]5.7812,[180]5.7722,[181]5.7539,[182]5.7361,[183]5.7222,[184]5.7206,[185]5.7127,[186]5.7143,[187]5.7189,[188]5.7177,[189]5.7333,[190]5.7336,[191]5.7516,[192]5.7694,[193]5.7870,[194]5.8005,[195]5.8229,[196]5.8370,[197]5.8560,[198]5.8707,[199]5.8731,[200]5.8767,[201]5.8695,[202]5.8852,[203]5.8939,[204]5.8915,[205]5.9027,[206]5.9065,[207]5.9048,[208]5.9137,[209]5.9162,[210]5.9222,[211]5.9318,[212]5.9375,[213]5.9474,[214]5.9488,[215]5.9519,[216]5.9657,[217]5.9819,[218]5.9959,[219]5.9956,[220]5.9929,[221]5.9855,[222]5.9832,[223]5.9747,[224]5.9654,[225]5.9603,[226]5.9801,[227]5.9845,[228]5.9901,[229]5.9955,[230]5.9911,[231]6.0054,[232]5.9939,[233]5.9785,[234]5.9625,[235]5.9387,[236]5.9324,[237]5.9205,[238]5.9226,[239]5.9091,[240]5.8983,[241]5.9007,[242]5.9023,[243]5.8983,[244]5.8871,[245]5.8834,[246]5.8722,[247]5.8614,[248]5.8541,[249]5.8503,[250]5.8535,[251]5.8452,[252]5.8407,[253]5.8313,[254]5.8254,[255]5.8153,[256]5.7985,[257]5.7872,[258]5.7789,[259]5.7792,[260]5.7711,[261]5.7654,[262]5.7597,[263]5.7537,[264]5.7294,[265]5.7289,[266]5.7255,[267]5.7192,[268]5.7272,[269]5.7261,[270]5.7275,[271]5.7338,[272]5.7369,[273]5.7372,[274]5.7386,[275]5.7451,[276]5.7514,[277]5.7669,[278]5.7758,[279]5.7858,[280]5.7887,[281]5.7994,[282]5.8042,[283]5.8191,[284]5.8286,[285]5.8379,[286]5.8511,[287]5.8506,[288]5.8568,[289]5.8492,[290]5.8335,[291]5.8190,[292]5.8030,[293]5.7893,[294]5.7891,[295]5.7884,[296]5.7927,[297]5.7906,[298]5.7921,[299]5.7882,[300]5.7770,[301]5.7762,[302]5.7686,[303]5.7611,[304]5.7520,[305]5.7480,[306]5.7357,[307]5.7374,[308]5.7383,[309]5.7225,[310]5.7184,[311]5.7126,[312]5.7130,[313]5.7070,[314]5.7038,[315]5.6882,[316]5.6831,[317]5.6684,[318]5.6490,[319]5.6621,[320]5.6751,[321]5.6801,[322]5.6761,[323]5.6705,[324]5.6696,[325]5.6809,[326]5.6814,[327]5.6834,[328]5.6871,[329]5.6931,[330]5.6956,[331]5.7076,[332]5.7036,[333]5.7112,[334]5.7046,[335]5.6986,[336]5.7005,[337]5.6989,[338]5.6990,[339]5.6943,[340]5.6904,[341]5.6974,[342]5.6992,[343]5.7036,[344]5.7031,[345]5.7034,[346]5.7006,[347]5.7031,[348]5.7065,[349]5.7095,[350]5.7075,[351]5.7078,[352]5.7078,[353]5.7018,[354]5.7007,[355]5.7045,[356]5.7073,[357]5.7050,[358]5.7133,[359]5.7159,[360]5.7137,[361]5.7134,[362]5.7205,[363]5.7318,[364]5.7379,[365]5.7430,[366]5.7455,[367]5.7536,[368]5.7506,[369]5.7520,[370]5.7536,[371]5.7488,[372]5.7542,[373]5.7582,[374]5.7570,[375]5.7559,[376]5.7630,[377]5.7596,[378]5.7627,[379]5.7672,[380]5.7598,[381]5.7570,[382]5.7519,[383]5.7500,[384]5.7491,[385]5.7479,[386]5.7486,[387]5.7486,[388]5.7438,[389]5.7395,[390]5.7335,[391]5.7268,[392]5.7221,[393]5.7221,[394]5.7246,[395]5.7226,[396]5.7150,[397]5.7221,[398]5.7263,[399]5.7334,[400]5.7325,[401]5.7346,[402]5.7355,[403]5.7376,[404]5.7436,[405]5.7348,[406]5.7311,[407]5.7305,[408]5.7327,[409]5.7437,[410]5.7543,[411]5.7634,[412]5.7782,[413]5.7893,[414]5.7953,[415]5.8011,[416]5.8082,[417]5.8199,[418]5.8238,[419]5.8290,[420]5.8372,[421]5.8476,[422]5.8515,[423]5.8570,[424]5.8666,[425]5.8747,[426]5.8814,[427]5.8848,[428]5.8920,[429]5.8972,[430]5.9040,[431]5.9180,[432]5.9217,[433]5.9202,[434]5.9164,[435]5.9178,[436]5.9206,[437]5.9299,[438]5.9373,[439]5.9337,[440]5.9316,[441]5.9271,[442]5.9261,[443]5.9276,[444]5.9285,[445]5.9268,[446]5.9288,[447]5.9310,[448]5.9346,[449]5.9323,[450]5.9326,[451]5.9291,[452]5.9147,[453]5.9052,[454]5.8990,[455]5.8994,[456]5.9043,[457]5.9059,[458]5.9042,[459]5.9040,[460]5.9119,[461]5.9082,[462]5.9066,[463]5.9098,[464]5.9089,[465]5.9068,[466]5.8998,[467]5.9012,[468]5.9013,[469]5.9031,[470]5.9039,[471]5.9001,[472]5.9039,[473]5.8982,[474]5.8993,[475]5.8931,[476]5.8951,[477]5.8880,[478]5.8871,[479]5.8922,[480]5.8969,[481]5.8994,[482]5.8953,[483]5.8916,[484]5.8931,[485]5.8917,[486]5.8873,[487]5.8866,[488]5.8846,[489]5.8805,[490]5.8788,[491]5.8762,[492]5.8709,[493]5.8677,[494]5.8656,[495]5.8636,[496]5.8601,[497]5.8554,[498]5.8538,[499]5.8494,[500]5.8408,[501]5.8354,[502]5.8354,[503]5.8337,[504]5.8254,[505]5.8273,[506]5.8279,[507]5.8226,[508]5.8183,[509]5.8184,[510]5.8205,[511]5.8251,[512]5.8285,[513]5.8301,[514]5.8358,[515]5.8310,[516]5.8306,[517]5.8308,[518]5.8302,[519]5.8326,[520]5.8349,[521]5.8361,[522]5.8387,[523]5.8390,[524]5.8441,[525]5.8473,[526]5.8487,[527]5.8500,[528]5.8453,[529]5.8456,[530]5.8411,[531]5.8392,[532]5.8442,[533]5.8463,[534]5.8452,[535]5.8484,[536]5.8431,[537]5.8419,[538]5.8464,[539]5.8477,[540]5.8497,[541]5.8505,[542]5.8510,[543]5.8533,[544]5.8545,[545]5.8532,[546]5.8544,[547]5.8505,[548]5.8457,[549]5.8452,[550]5.8427,[551]5.8401,[552]5.8387,[553]5.8351,[554]5.8328,[555]5.8298,[556]5.8294,[557]5.8317,[558]5.8282,[559]5.8286,[560]5.8287,[561]5.8289,[562]5.8263,[563]5.8261,[564]5.8307,[565]5.8318,[566]5.8324,[567]5.8296,[568]5.8299,[569]5.8285,[570]5.8313,[571]5.8321,[572]5.8327,[573]5.8324,[574]5.8287,[575]5.8272,[576]5.8271,[577]5.8256,[578]5.8240,[579]5.8241,[580]5.8187,[581]5.8159,[582]5.8151,[583]5.8164,[584]5.8165,[585]5.8093,[586]5.8029,[587]5.8026,[588]5.8066,[589]5.8117,[590]5.8145,[591]5.8164,[592]5.8151,[593]5.8111,[594]5.8115,[595]5.8097,[596]5.8135,[597]5.8110,[598]5.8078,[599]5.8097,[600]5.8087,[601]5.8073,[602]5.8071,[603]5.8087,[604]5.8101,[605]5.8134,[606]5.8151,[607]5.8141,[608]5.8100,[609]5.8110,[610]5.8148,[611]5.8136,[612]5.8158,[613]5.8132,[614]5.8086,[615]5.8023,[616]5.8048,[617]5.7991,[618]5.7942,[619]5.7893,[620]5.7764,[621]5.7701,[622]5.7680,[623]5.7699,[624]5.7702,[625]5.7711,[626]5.7706,[627]5.7734,[628]5.7741,[629]5.7745,[630]5.7776,[631]5.7829,[632]5.7884,[633]5.7880,[634]5.7909,[635]5.7921,[636]5.7893,[637]5.7855,[638]5.7879,[639]5.7840,[640]5.7852,[641]5.7855,[642]5.7912,[643]5.7928,[644]5.7938,[645]5.7925,[646]5.7961,[647]5.7922,[648]5.7929,[649]5.7936,[650]5.7970,[651]5.8013,[652]5.8021,[653]5.8056,[654]5.7995,[655]5.7986,
llama_print_timings:        load time =    3540.07 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =   58791.53 ms / 335360 tokens (    0.18 ms per token,  5704.22 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =   83780.78 ms / 335361 tokens

Final estimate: PPL = 5.7986 +/- 0.03237
