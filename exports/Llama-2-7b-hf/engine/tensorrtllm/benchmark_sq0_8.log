[BENCHMARK] model_name llama_7b world_size 1 num_heads 32 num_kv_heads 32 num_layers 32 hidden_size 4096 vocab_size 32000 precision float16 batch_size 1 input_length 128 output_length 128 gpu_peak_mem(gb) 15.37 build_time(s) 0 tokens_per_sec 129.06 percentile95(ms) 994.28 percentile99(ms) 1004.085 latency(ms) 991.824 compute_cap sm80
