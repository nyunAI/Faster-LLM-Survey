You passed `quantization_config` to `from_pretrained` but the model you're loading already has a `quantization_config` attribute and has already quantized weights. However, loading attributes (e.g. ['use_cuda_fp16', 'use_exllama', 'max_input_length', 'exllama_config', 'disable_exllama']) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.
BASE: /home/shwu/LLM-Efficiency-Survey
MODEL: /home/shwu/LLM-Efficiency-Survey/model
MODEL_PATH: /home/shwu/LLM-Efficiency-Survey/model/Llama-2-7b-hf
METHOD_EXPORTS: /home/shwu/LLM-Efficiency-Survey/exports/Llama-2-7b-hf/quant/hf


=================================== Quant: gptq_4_exllama_version_two ===================================
Output: What is the meaning of life? Why are we here? What is the meaning of our existence? Why are we born? Why are we dying? Why do we suffer? Why do we suffer from pain and sorrow? Why do we suffer from diseases? Why do we suffer from injustice? Why do we suffer from poverty? Why do we suffer from wars? Why do we suffer from natural disasters? Why do we suffer from depression? Why do we suffer from anxiety? Why do we suffer from fear? Why do we suffer from loneliness? Why do we suffer from addiction? Why do we suffer from mental illness?
======================================== Stats ========================================
Generated 128 tokens in 4.195513725280762 seconds (30.508778753055875 tokens/s)
================================================================================


