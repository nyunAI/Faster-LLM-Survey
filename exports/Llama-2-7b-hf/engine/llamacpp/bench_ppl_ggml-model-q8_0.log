main: build = 1833 (326b418b)
main: built with cc (Debian 10.2.1-6) 10.2.1 20210110 for x86_64-linux-gnu
main: seed  = 1706107092
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from exports/Llama-2-7b-hf/engine/llamacpp/ggml-model-q8_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 7
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q8_0:  226 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q8_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 6.67 GiB (8.50 BPW) 
llm_load_print_meta: general.name     = LLaMA v2
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size       =    0.11 MiB
llm_load_tensors: using CUDA for GPU acceleration
llm_load_tensors: system memory used  =  132.92 MiB
llm_load_tensors: VRAM used           = 6695.83 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
....................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init: VRAM kv self = 256.00 MB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_build_graph: non-view tensors processed: 676/676
llama_new_context_with_model: compute buffer total size = 73.69 MiB
llama_new_context_with_model: VRAM scratch buffer: 70.50 MiB
llama_new_context_with_model: total VRAM used: 7022.33 MiB (model: 6695.83 MiB, context: 326.50 MiB)

system_info: n_threads = 32 / 12 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 1076.68 ms
perplexity: calculating perplexity over 655 chunks, batch_size=512
perplexity: 0.19 seconds per pass - ETA 2.05 minutes
[1]4.1747,[2]4.7002,[3]5.3509,[4]5.9195,[5]6.0435,[6]5.9614,[7]6.1327,[8]6.2222,[9]6.5444,[10]6.7253,[11]6.9429,[12]6.9920,[13]6.9167,[14]6.9928,[15]7.2149,[16]6.8749,[17]6.7595,[18]6.7508,[19]6.4306,[20]6.4251,[21]6.3505,[22]6.1814,[23]6.1516,[24]6.0611,[25]6.0472,[26]5.8919,[27]5.7101,[28]5.6100,[29]5.5278,[30]5.3777,[31]5.3401,[32]5.3594,[33]5.3157,[34]5.3448,[35]5.3604,[36]5.3849,[37]5.3792,[38]5.3777,[39]5.3914,[40]5.4410,[41]5.4625,[42]5.4999,[43]5.4617,[44]5.5159,[45]5.5254,[46]5.5023,[47]5.5264,[48]5.5058,[49]5.5066,[50]5.4741,[51]5.4740,[52]5.4640,[53]5.5111,[54]5.4984,[55]5.4836,[56]5.5132,[57]5.5320,[58]5.5601,[59]5.5814,[60]5.6305,[61]5.6270,[62]5.6879,[63]5.7229,[64]5.7294,[65]5.7714,[66]5.7820,[67]5.8011,[68]5.8212,[69]5.8560,[70]5.8937,[71]5.9203,[72]5.9551,[73]6.0068,[74]6.0168,[75]6.0279,[76]6.0451,[77]6.0599,[78]6.0493,[79]6.0757,[80]6.0747,[81]6.0897,[82]6.0950,[83]6.0464,[84]6.0363,[85]6.0324,[86]6.0153,[87]5.9587,[88]5.9350,[89]5.9206,[90]5.9132,[91]5.9335,[92]5.9328,[93]5.9345,[94]5.9346,[95]5.9637,[96]5.9624,[97]5.9547,[98]5.9491,[99]5.9378,[100]5.9402,[101]5.9628,[102]5.9584,[103]5.9752,[104]5.9843,[105]5.9861,[106]6.0030,[107]6.0053,[108]6.0200,[109]6.0190,[110]6.0155,[111]6.0350,[112]6.0547,[113]6.0571,[114]6.0576,[115]6.0661,[116]6.0547,[117]6.0589,[118]6.0838,[119]6.1042,[120]6.1374,[121]6.1537,[122]6.1757,[123]6.2161,[124]6.2333,[125]6.2248,[126]6.2606,[127]6.2952,[128]6.3203,[129]6.3044,[130]6.3134,[131]6.3077,[132]6.2989,[133]6.2845,[134]6.2920,[135]6.2897,[136]6.2783,[137]6.2722,[138]6.2553,[139]6.2484,[140]6.2450,[141]6.2199,[142]6.2154,[143]6.1877,[144]6.1684,[145]6.1595,[146]6.1494,[147]6.1561,[148]6.1575,[149]6.1506,[150]6.1507,[151]6.1569,[152]6.1504,[153]6.1372,[154]6.1298,[155]6.1370,[156]6.1347,[157]6.1497,[158]6.1525,[159]6.1544,[160]6.1569,[161]6.1697,[162]6.1423,[163]6.1309,[164]6.1062,[165]6.0757,[166]6.0481,[167]6.0109,[168]5.9803,[169]5.9662,[170]5.9550,[171]5.9301,[172]5.9165,[173]5.9013,[174]5.8732,[175]5.8526,[176]5.8391,[177]5.8194,[178]5.7980,[179]5.7823,[180]5.7733,[181]5.7549,[182]5.7373,[183]5.7232,[184]5.7215,[185]5.7137,[186]5.7154,[187]5.7202,[188]5.7190,[189]5.7345,[190]5.7348,[191]5.7528,[192]5.7707,[193]5.7883,[194]5.8017,[195]5.8241,[196]5.8383,[197]5.8573,[198]5.8721,[199]5.8746,[200]5.8781,[201]5.8710,[202]5.8867,[203]5.8954,[204]5.8928,[205]5.9040,[206]5.9080,[207]5.9064,[208]5.9153,[209]5.9176,[210]5.9236,[211]5.9331,[212]5.9387,[213]5.9485,[214]5.9498,[215]5.9528,[216]5.9668,[217]5.9830,[218]5.9970,[219]5.9967,[220]5.9940,[221]5.9866,[222]5.9842,[223]5.9757,[224]5.9664,[225]5.9614,[226]5.9811,[227]5.9855,[228]5.9912,[229]5.9965,[230]5.9922,[231]6.0064,[232]5.9949,[233]5.9794,[234]5.9634,[235]5.9398,[236]5.9335,[237]5.9216,[238]5.9237,[239]5.9102,[240]5.8995,[241]5.9020,[242]5.9036,[243]5.8997,[244]5.8885,[245]5.8848,[246]5.8737,[247]5.8628,[248]5.8554,[249]5.8517,[250]5.8549,[251]5.8466,[252]5.8421,[253]5.8326,[254]5.8267,[255]5.8166,[256]5.7998,[257]5.7885,[258]5.7802,[259]5.7805,[260]5.7724,[261]5.7668,[262]5.7611,[263]5.7552,[264]5.7308,[265]5.7303,[266]5.7269,[267]5.7206,[268]5.7287,[269]5.7275,[270]5.7288,[271]5.7352,[272]5.7383,[273]5.7387,[274]5.7401,[275]5.7465,[276]5.7529,[277]5.7683,[278]5.7772,[279]5.7872,[280]5.7901,[281]5.8009,[282]5.8057,[283]5.8207,[284]5.8301,[285]5.8394,[286]5.8525,[287]5.8519,[288]5.8582,[289]5.8505,[290]5.8348,[291]5.8203,[292]5.8043,[293]5.7905,[294]5.7903,[295]5.7897,[296]5.7940,[297]5.7919,[298]5.7933,[299]5.7893,[300]5.7781,[301]5.7773,[302]5.7697,[303]5.7621,[304]5.7531,[305]5.7490,[306]5.7367,[307]5.7385,[308]5.7393,[309]5.7235,[310]5.7194,[311]5.7136,[312]5.7141,[313]5.7081,[314]5.7049,[315]5.6894,[316]5.6844,[317]5.6696,[318]5.6502,[319]5.6633,[320]5.6763,[321]5.6813,[322]5.6773,[323]5.6716,[324]5.6708,[325]5.6822,[326]5.6827,[327]5.6848,[328]5.6884,[329]5.6944,[330]5.6970,[331]5.7089,[332]5.7050,[333]5.7125,[334]5.7060,[335]5.7000,[336]5.7019,[337]5.7004,[338]5.7005,[339]5.6957,[340]5.6918,[341]5.6988,[342]5.7006,[343]5.7050,[344]5.7044,[345]5.7047,[346]5.7020,[347]5.7044,[348]5.7079,[349]5.7107,[350]5.7088,[351]5.7090,[352]5.7090,[353]5.7030,[354]5.7019,[355]5.7057,[356]5.7085,[357]5.7061,[358]5.7144,[359]5.7170,[360]5.7147,[361]5.7144,[362]5.7215,[363]5.7329,[364]5.7390,[365]5.7440,[366]5.7465,[367]5.7546,[368]5.7517,[369]5.7530,[370]5.7546,[371]5.7498,[372]5.7553,[373]5.7593,[374]5.7581,[375]5.7569,[376]5.7640,[377]5.7606,[378]5.7636,[379]5.7682,[380]5.7607,[381]5.7579,[382]5.7528,[383]5.7510,[384]5.7500,[385]5.7488,[386]5.7495,[387]5.7496,[388]5.7447,[389]5.7404,[390]5.7344,[391]5.7277,[392]5.7230,[393]5.7230,[394]5.7255,[395]5.7235,[396]5.7159,[397]5.7230,[398]5.7271,[399]5.7342,[400]5.7333,[401]5.7354,[402]5.7363,[403]5.7384,[404]5.7444,[405]5.7355,[406]5.7319,[407]5.7313,[408]5.7334,[409]5.7444,[410]5.7551,[411]5.7642,[412]5.7791,[413]5.7901,[414]5.7961,[415]5.8018,[416]5.8090,[417]5.8207,[418]5.8246,[419]5.8298,[420]5.8380,[421]5.8484,[422]5.8523,[423]5.8578,[424]5.8674,[425]5.8755,[426]5.8822,[427]5.8856,[428]5.8928,[429]5.8980,[430]5.9048,[431]5.9189,[432]5.9225,[433]5.9211,[434]5.9173,[435]5.9188,[436]5.9215,[437]5.9309,[438]5.9381,[439]5.9345,[440]5.9324,[441]5.9278,[442]5.9269,[443]5.9284,[444]5.9293,[445]5.9275,[446]5.9295,[447]5.9317,[448]5.9354,[449]5.9331,[450]5.9334,[451]5.9299,[452]5.9155,[453]5.9060,[454]5.8998,[455]5.9002,[456]5.9051,[457]5.9067,[458]5.9049,[459]5.9048,[460]5.9127,[461]5.9090,[462]5.9074,[463]5.9106,[464]5.9097,[465]5.9075,[466]5.9006,[467]5.9019,[468]5.9020,[469]5.9038,[470]5.9045,[471]5.9008,[472]5.9046,[473]5.8990,[474]5.9001,[475]5.8938,[476]5.8958,[477]5.8887,[478]5.8878,[479]5.8929,[480]5.8976,[481]5.9000,[482]5.8960,[483]5.8923,[484]5.8938,[485]5.8924,[486]5.8881,[487]5.8873,[488]5.8854,[489]5.8812,[490]5.8796,[491]5.8770,[492]5.8717,[493]5.8684,[494]5.8664,[495]5.8644,[496]5.8609,[497]5.8562,[498]5.8545,[499]5.8502,[500]5.8416,[501]5.8361,[502]5.8361,[503]5.8345,[504]5.8262,[505]5.8281,[506]5.8286,[507]5.8233,[508]5.8191,[509]5.8192,[510]5.8213,[511]5.8260,[512]5.8293,[513]5.8310,[514]5.8367,[515]5.8318,[516]5.8315,[517]5.8317,[518]5.8311,[519]5.8334,[520]5.8358,[521]5.8370,[522]5.8395,[523]5.8398,[524]5.8450,[525]5.8482,[526]5.8495,[527]5.8509,[528]5.8461,[529]5.8465,[530]5.8420,[531]5.8401,[532]5.8451,[533]5.8472,[534]5.8461,[535]5.8493,[536]5.8440,[537]5.8428,[538]5.8473,[539]5.8486,[540]5.8506,[541]5.8513,[542]5.8519,[543]5.8542,[544]5.8554,[545]5.8540,[546]5.8553,[547]5.8514,[548]5.8466,[549]5.8461,[550]5.8436,[551]5.8411,[552]5.8397,[553]5.8361,[554]5.8337,[555]5.8307,[556]5.8303,[557]5.8326,[558]5.8291,[559]5.8295,[560]5.8296,[561]5.8298,[562]5.8273,[563]5.8270,[564]5.8316,[565]5.8327,[566]5.8333,[567]5.8305,[568]5.8308,[569]5.8295,[570]5.8322,[571]5.8330,[572]5.8335,[573]5.8333,[574]5.8296,[575]5.8281,[576]5.8280,[577]5.8265,[578]5.8249,[579]5.8250,[580]5.8197,[581]5.8169,[582]5.8161,[583]5.8173,[584]5.8174,[585]5.8103,[586]5.8039,[587]5.8035,[588]5.8075,[589]5.8126,[590]5.8154,[591]5.8173,[592]5.8159,[593]5.8120,[594]5.8124,[595]5.8105,[596]5.8143,[597]5.8119,[598]5.8087,[599]5.8106,[600]5.8096,[601]5.8082,[602]5.8080,[603]5.8095,[604]5.8109,[605]5.8142,[606]5.8160,[607]5.8149,[608]5.8108,[609]5.8118,[610]5.8157,[611]5.8145,[612]5.8167,[613]5.8141,[614]5.8094,[615]5.8031,[616]5.8057,[617]5.7999,[618]5.7950,[619]5.7901,[620]5.7773,[621]5.7710,[622]5.7689,[623]5.7707,[624]5.7711,[625]5.7719,[626]5.7714,[627]5.7742,[628]5.7749,[629]5.7753,[630]5.7785,[631]5.7838,[632]5.7894,[633]5.7890,[634]5.7919,[635]5.7931,[636]5.7903,[637]5.7865,[638]5.7888,[639]5.7850,[640]5.7861,[641]5.7865,[642]5.7921,[643]5.7938,[644]5.7948,[645]5.7935,[646]5.7971,[647]5.7932,[648]5.7939,[649]5.7947,[650]5.7980,[651]5.8024,[652]5.8032,[653]5.8066,[654]5.8005,[655]5.7996,
llama_print_timings:        load time =    1976.81 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =   70088.83 ms / 335360 tokens (    0.21 ms per token,  4784.79 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =   94620.08 ms / 335361 tokens

Final estimate: PPL = 5.7996 +/- 0.03237
