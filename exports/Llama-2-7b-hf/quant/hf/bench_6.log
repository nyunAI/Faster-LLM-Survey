You passed `quantization_config` to `from_pretrained` but the model you're loading already has a `quantization_config` attribute and has already quantized weights. However, loading attributes (e.g. ['use_cuda_fp16', 'use_exllama', 'max_input_length', 'exllama_config', 'disable_exllama']) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.
BASE: /home/shwu/LLM-Efficiency-Survey
MODEL: /home/shwu/LLM-Efficiency-Survey/model
MODEL_PATH: /home/shwu/LLM-Efficiency-Survey/model/Llama-2-7b-hf
METHOD_EXPORTS: /home/shwu/LLM-Efficiency-Survey/exports/Llama-2-7b-hf/quant/hf
Quant: gptq_4
Memory footprint: 3.6914825439453125 GiB
Output: What is the meaning of life? I have always found this question to be a very difficult one to answer. everybody has a different meaning of life. some people believe that the meaning of life is to live a life full of happiness and joy. others believe that the meaning of life is to live a life full of sorrow and pain.
I personally believe that the meaning of life is to live a life full of love. I believe that the meaning of life is to live a life full of love because I believe that the meaning of life is to live a life full of love. I believe that the meaning of life is to live a life full of love because I believe
----------------------------------- Stats -----------------------------------
Generated 128 tokens in 7.052340269088745 seconds (18.15000341957965 tokens/s)
--------------------------------------------------------------------------------


