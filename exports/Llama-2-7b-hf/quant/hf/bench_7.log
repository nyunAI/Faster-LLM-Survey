You passed `quantization_config` to `from_pretrained` but the model you're loading already has a `quantization_config` attribute and has already quantized weights. However, loading attributes (e.g. ['use_cuda_fp16', 'use_exllama', 'max_input_length', 'exllama_config', 'disable_exllama']) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.
BASE: /home/shwu/LLM-Efficiency-Survey
MODEL: /home/shwu/LLM-Efficiency-Survey/model
MODEL_PATH: /home/shwu/LLM-Efficiency-Survey/model/Llama-2-7b-hf
METHOD_EXPORTS: /home/shwu/LLM-Efficiency-Survey/exports/Llama-2-7b-hf/quant/hf


=================================== Quant: gptq_4 ===================================
Output: What is the meaning of life? What is the meaning of the universe? What is the meaning of death?
 everyone wants to know the answers to these questions.
I’m no different. But unlike most people, I’ve spent a lot of time thinking about these questions. I’ve read books, watched documentaries, and talked with people who have spent a lot of time thinking about these questions.
And I’ve come to the conclusion that there is no single answer to these questions.
The meaning of life is whatever you want it to be.
The meaning of the universe is whatever you want it to be.
And the meaning of
======================================== Stats ========================================
Generated 128 tokens in 5.918351411819458 seconds (21.62764443901945 tokens/s)
================================================================================


