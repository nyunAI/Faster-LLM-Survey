2024-01-18 09:04:51 INFO [__main__] max_memory: {0: '40GIB'}
2024-01-18 09:04:51 INFO [__main__] loading model and tokenizer
2024-01-18 09:04:52 INFO [auto_gptq.modeling._base] lm_head not been quantized, will be ignored when make_quant.
2024-01-18 09:04:56 WARNING [auto_gptq.nn_modules.fused_llama_mlp] Skipping module injection for FusedLlamaMLPForQuantizedModel as currently not supported with use_triton=False.
2024-01-18 09:05:40 INFO [__main__] model and tokenizer loading time: 49.0058s
2024-01-18 09:05:40 INFO [__main__] model quantized: True
2024-01-18 09:05:40 INFO [__main__] quantize config: {'bits': 8, 'group_size': 128, 'damp_percent': 0.01, 'desc_act': False, 'static_groups': False, 'sym': True, 'true_sequential': True, 'model_name_or_path': 'exports/Llama-2-7b-hf/quant/gptq/q8_128g', 'model_file_base_name': 'gptq_model-8bit-128g'}
2024-01-18 09:05:40 INFO [__main__] model device map: OrderedDict([('', 0)])
2024-01-18 09:05:40 INFO [__main__] loading data
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Map: 100%|██████████| 10/10 [00:00<00:00, 602.35 examples/s]
2024-01-18 09:05:41 INFO [__main__] generation config: {'max_length': 20, 'max_new_tokens': 128, 'min_length': 0, 'min_new_tokens': 128, 'early_stopping': False, 'max_time': None, 'do_sample': False, 'num_beams': 4, 'num_beam_groups': 1, 'penalty_alpha': None, 'use_cache': True, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'epsilon_cutoff': 0.0, 'eta_cutoff': 0.0, 'diversity_penalty': 0.0, 'repetition_penalty': 1.0, 'encoder_repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'bad_words_ids': None, 'force_words_ids': None, 'renormalize_logits': False, 'constraints': None, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'forced_decoder_ids': None, 'sequence_bias': None, 'guidance_scale': None, 'low_memory': None, 'num_return_sequences': 4, 'output_attentions': False, 'output_hidden_states': False, 'output_scores': False, 'return_dict_in_generate': False, 'pad_token_id': 2, 'bos_token_id': None, 'eos_token_id': None, 'encoder_no_repeat_ngram_size': 0, 'decoder_start_token_id': None, 'num_assistant_tokens': 5, 'num_assistant_tokens_schedule': 'heuristic', 'generation_kwargs': {}, '_from_model_config': False, 'transformers_version': '4.36.2'}
2024-01-18 09:05:41 INFO [__main__] benchmark generation speed
  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:06<?, ?it/s, num_tokens=512, speed=73.7697tokens/s, time=6.94] 10%|█         | 1/10 [00:06<01:02,  6.96s/it, num_tokens=512, speed=73.7697tokens/s, time=6.94] 10%|█         | 1/10 [00:12<01:02,  6.96s/it, num_tokens=512, speed=86.2562tokens/s, time=5.94] 20%|██        | 2/10 [00:12<00:50,  6.36s/it, num_tokens=512, speed=86.2562tokens/s, time=5.94] 20%|██        | 2/10 [00:18<00:50,  6.36s/it, num_tokens=512, speed=92.8908tokens/s, time=5.51] 30%|███       | 3/10 [00:18<00:41,  5.98s/it, num_tokens=512, speed=92.8908tokens/s, time=5.51] 30%|███       | 3/10 [00:23<00:41,  5.98s/it, num_tokens=512, speed=92.7480tokens/s, time=5.52] 40%|████      | 4/10 [00:23<00:34,  5.81s/it, num_tokens=512, speed=92.7480tokens/s, time=5.52] 40%|████      | 4/10 [00:29<00:34,  5.81s/it, num_tokens=512, speed=89.2013tokens/s, time=5.74] 50%|█████     | 5/10 [00:29<00:28,  5.79s/it, num_tokens=512, speed=89.2013tokens/s, time=5.74] 50%|█████     | 5/10 [00:35<00:28,  5.79s/it, num_tokens=512, speed=92.5673tokens/s, time=5.53] 60%|██████    | 6/10 [00:35<00:22,  5.70s/it, num_tokens=512, speed=92.5673tokens/s, time=5.53] 60%|██████    | 6/10 [00:40<00:22,  5.70s/it, num_tokens=512, speed=89.7669tokens/s, time=5.7]  70%|███████   | 7/10 [00:40<00:17,  5.71s/it, num_tokens=512, speed=89.7669tokens/s, time=5.7] 70%|███████   | 7/10 [00:46<00:17,  5.71s/it, num_tokens=512, speed=90.0136tokens/s, time=5.69] 80%|████████  | 8/10 [00:46<00:11,  5.71s/it, num_tokens=512, speed=90.0136tokens/s, time=5.69] 80%|████████  | 8/10 [00:52<00:11,  5.71s/it, num_tokens=512, speed=85.6012tokens/s, time=5.98] 90%|█████████ | 9/10 [00:52<00:05,  5.80s/it, num_tokens=512, speed=85.6012tokens/s, time=5.98] 90%|█████████ | 9/10 [00:58<00:05,  5.80s/it, num_tokens=512, speed=94.2718tokens/s, time=5.43]100%|██████████| 10/10 [00:58<00:00,  5.69s/it, num_tokens=512, speed=94.2718tokens/s, time=5.43]100%|██████████| 10/10 [00:58<00:00,  5.81s/it, num_tokens=512, speed=94.2718tokens/s, time=5.43]
2024-01-18 09:06:39 INFO [__main__] generated 5120 tokens using 57.98347353935242 seconds, generation speed: 88.30102247194868 tokens/s
