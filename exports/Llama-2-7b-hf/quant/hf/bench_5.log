You passed `quantization_config` to `from_pretrained` but the model you're loading already has a `quantization_config` attribute and has already quantized weights. However, loading attributes (e.g. ['use_cuda_fp16', 'use_exllama', 'max_input_length', 'exllama_config', 'disable_exllama']) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.
BASE: /home/shwu/LLM-Efficiency-Survey
MODEL: /home/shwu/LLM-Efficiency-Survey/model
MODEL_PATH: /home/shwu/LLM-Efficiency-Survey/model/Llama-2-7b-hf
METHOD_EXPORTS: /home/shwu/LLM-Efficiency-Survey/exports/Llama-2-7b-hf/quant/hf
Quant: gptq_3
Memory footprint: 2.9316864013671875 GiB
Output: What is the meaning of life?
 
**Lorem ipsum dolor sit amet**, consectetur adipisci pene proin nisi quam pro quo nulla. Duis aut rixi ut a quo nulla, nisi quam pro quo nulla. Duis aut rixi ut a quo nulla, nisi quam pro quo nulla. Duis aut rixi ut a quo nulla, nisi quam pro quo nulla. Duis aut rixi ut a quo nulla, nisi quam pro quo nulla. Duis
----------------------------------- Stats -----------------------------------
Generated 128 tokens in 6.916422128677368 seconds (18.50667839796492 tokens/s)
--------------------------------------------------------------------------------


