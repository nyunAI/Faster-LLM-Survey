main: build = 1833 (326b418b)
main: built with cc (Debian 10.2.1-6) 10.2.1 20210110 for x86_64-linux-gnu
main: seed  = 1706107196
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from exports/Llama-2-7b-hf/engine/llamacpp/ggml-model-q2_k.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 10
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q2_K:  129 tensors
llama_model_loader: - type q3_K:   96 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q2_K - Medium
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 2.36 GiB (3.01 BPW) 
llm_load_print_meta: general.name     = LLaMA v2
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size       =    0.11 MiB
llm_load_tensors: using CUDA for GPU acceleration
llm_load_tensors: system memory used  =   41.13 MiB
llm_load_tensors: VRAM used           = 2373.80 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
.................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init: VRAM kv self = 256.00 MB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_build_graph: non-view tensors processed: 676/676
llama_new_context_with_model: compute buffer total size = 73.69 MiB
llama_new_context_with_model: VRAM scratch buffer: 70.50 MiB
llama_new_context_with_model: total VRAM used: 2700.30 MiB (model: 2373.80 MiB, context: 326.50 MiB)

system_info: n_threads = 32 / 12 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 1069.06 ms
perplexity: calculating perplexity over 655 chunks, batch_size=512
perplexity: 0.19 seconds per pass - ETA 2.10 minutes
[1]4.9690,[2]5.9437,[3]6.7977,[4]7.5454,[5]7.5834,[6]7.3592,[7]7.5106,[8]7.5446,[9]7.8708,[10]8.1156,[11]8.3595,[12]8.3919,[13]8.3006,[14]8.4264,[15]8.6739,[16]8.2428,[17]8.0681,[18]8.1169,[19]7.7091,[20]7.6734,[21]7.5716,[22]7.3987,[23]7.3552,[24]7.2471,[25]7.2453,[26]7.0525,[27]6.8179,[28]6.6986,[29]6.6048,[30]6.4176,[31]6.3821,[32]6.4106,[33]6.3568,[34]6.3910,[35]6.4075,[36]6.4512,[37]6.4571,[38]6.4678,[39]6.4913,[40]6.5425,[41]6.5673,[42]6.6156,[43]6.5688,[44]6.6221,[45]6.6362,[46]6.6062,[47]6.6336,[48]6.5998,[49]6.6064,[50]6.5520,[51]6.5441,[52]6.5251,[53]6.5705,[54]6.5498,[55]6.5312,[56]6.5723,[57]6.5910,[58]6.6180,[59]6.6349,[60]6.6953,[61]6.6865,[62]6.7570,[63]6.7946,[64]6.7992,[65]6.8518,[66]6.8580,[67]6.8771,[68]6.9098,[69]6.9469,[70]6.9936,[71]7.0285,[72]7.0706,[73]7.1350,[74]7.1385,[75]7.1476,[76]7.1674,[77]7.1847,[78]7.1836,[79]7.2174,[80]7.2115,[81]7.2306,[82]7.2378,[83]7.1781,[84]7.1675,[85]7.1641,[86]7.1357,[87]7.0868,[88]7.0579,[89]7.0328,[90]7.0241,[91]7.0551,[92]7.0578,[93]7.0589,[94]7.0567,[95]7.0882,[96]7.0866,[97]7.0826,[98]7.0713,[99]7.0512,[100]7.0521,[101]7.0785,[102]7.0741,[103]7.0993,[104]7.1060,[105]7.1020,[106]7.1257,[107]7.1297,[108]7.1463,[109]7.1440,[110]7.1381,[111]7.1626,[112]7.1928,[113]7.1948,[114]7.1993,[115]7.2058,[116]7.1951,[117]7.1966,[118]7.2227,[119]7.2472,[120]7.2881,[121]7.3076,[122]7.3278,[123]7.3730,[124]7.3913,[125]7.3775,[126]7.4308,[127]7.4780,[128]7.5043,[129]7.4842,[130]7.4871,[131]7.4805,[132]7.4695,[133]7.4562,[134]7.4718,[135]7.4706,[136]7.4627,[137]7.4561,[138]7.4362,[139]7.4313,[140]7.4322,[141]7.4175,[142]7.4138,[143]7.3907,[144]7.3716,[145]7.3683,[146]7.3592,[147]7.3714,[148]7.3732,[149]7.3653,[150]7.3645,[151]7.3716,[152]7.3606,[153]7.3410,[154]7.3326,[155]7.3377,[156]7.3348,[157]7.3569,[158]7.3592,[159]7.3640,[160]7.3699,[161]7.3869,[162]7.3532,[163]7.3362,[164]7.3067,[165]7.2676,[166]7.2311,[167]7.1825,[168]7.1465,[169]7.1289,[170]7.1162,[171]7.0832,[172]7.0635,[173]7.0460,[174]7.0144,[175]6.9866,[176]6.9699,[177]6.9440,[178]6.9196,[179]6.8972,[180]6.8886,[181]6.8677,[182]6.8433,[183]6.8243,[184]6.8194,[185]6.8091,[186]6.8138,[187]6.8186,[188]6.8175,[189]6.8347,[190]6.8363,[191]6.8570,[192]6.8756,[193]6.8967,[194]6.9115,[195]6.9351,[196]6.9515,[197]6.9749,[198]6.9911,[199]6.9946,[200]6.9963,[201]6.9916,[202]7.0168,[203]7.0267,[204]7.0378,[205]7.0507,[206]7.0579,[207]7.0521,[208]7.0632,[209]7.0643,[210]7.0691,[211]7.0784,[212]7.0831,[213]7.0934,[214]7.0985,[215]7.1030,[216]7.1185,[217]7.1357,[218]7.1525,[219]7.1525,[220]7.1469,[221]7.1369,[222]7.1320,[223]7.1206,[224]7.1102,[225]7.1054,[226]7.1260,[227]7.1363,[228]7.1476,[229]7.1535,[230]7.1513,[231]7.1685,[232]7.1560,[233]7.1358,[234]7.1171,[235]7.1023,[236]7.0931,[237]7.0796,[238]7.0810,[239]7.0629,[240]7.0505,[241]7.0533,[242]7.0558,[243]7.0497,[244]7.0388,[245]7.0365,[246]7.0231,[247]7.0082,[248]6.9981,[249]6.9941,[250]6.9986,[251]6.9888,[252]6.9849,[253]6.9734,[254]6.9696,[255]6.9561,[256]6.9365,[257]6.9240,[258]6.9136,[259]6.9098,[260]6.8991,[261]6.8929,[262]6.8865,[263]6.8818,[264]6.8648,[265]6.8649,[266]6.8605,[267]6.8532,[268]6.8648,[269]6.8653,[270]6.8655,[271]6.8738,[272]6.8788,[273]6.8796,[274]6.8811,[275]6.8903,[276]6.8971,[277]6.9135,[278]6.9260,[279]6.9372,[280]6.9398,[281]6.9515,[282]6.9574,[283]6.9738,[284]6.9846,[285]6.9935,[286]7.0126,[287]7.0109,[288]7.0180,[289]7.0080,[290]6.9898,[291]6.9737,[292]6.9564,[293]6.9385,[294]6.9390,[295]6.9379,[296]6.9410,[297]6.9402,[298]6.9413,[299]6.9368,[300]6.9247,[301]6.9213,[302]6.9132,[303]6.9039,[304]6.8930,[305]6.8898,[306]6.8738,[307]6.8748,[308]6.8769,[309]6.8592,[310]6.8535,[311]6.8480,[312]6.8481,[313]6.8401,[314]6.8376,[315]6.8191,[316]6.8169,[317]6.7983,[318]6.7755,[319]6.7912,[320]6.8071,[321]6.8124,[322]6.8066,[323]6.8025,[324]6.8042,[325]6.8166,[326]6.8154,[327]6.8181,[328]6.8219,[329]6.8300,[330]6.8358,[331]6.8503,[332]6.8461,[333]6.8725,[334]6.8661,[335]6.8578,[336]6.8595,[337]6.8562,[338]6.8559,[339]6.8504,[340]6.8470,[341]6.8537,[342]6.8551,[343]6.8609,[344]6.8602,[345]6.8581,[346]6.8549,[347]6.8573,[348]6.8623,[349]6.8641,[350]6.8622,[351]6.8627,[352]6.8632,[353]6.8573,[354]6.8558,[355]6.8590,[356]6.8624,[357]6.8579,[358]6.8678,[359]6.8725,[360]6.8674,[361]6.8651,[362]6.8714,[363]6.8851,[364]6.8927,[365]6.8990,[366]6.9012,[367]6.9100,[368]6.9049,[369]6.9063,[370]6.9085,[371]6.9023,[372]6.9071,[373]6.9124,[374]6.9111,[375]6.9080,[376]6.9187,[377]6.9131,[378]6.9156,[379]6.9216,[380]6.9109,[381]6.9068,[382]6.9018,[383]6.8986,[384]6.8969,[385]6.8950,[386]6.8953,[387]6.8950,[388]6.8881,[389]6.8818,[390]6.8736,[391]6.8649,[392]6.8615,[393]6.8619,[394]6.8652,[395]6.8629,[396]6.8525,[397]6.8619,[398]6.8661,[399]6.8739,[400]6.8728,[401]6.8741,[402]6.8750,[403]6.8767,[404]6.8817,[405]6.8753,[406]6.8710,[407]6.8719,[408]6.8740,[409]6.8861,[410]6.8984,[411]6.9114,[412]6.9301,[413]6.9450,[414]6.9527,[415]6.9595,[416]6.9683,[417]6.9827,[418]6.9878,[419]6.9939,[420]7.0030,[421]7.0147,[422]7.0182,[423]7.0260,[424]7.0381,[425]7.0479,[426]7.0553,[427]7.0589,[428]7.0663,[429]7.0724,[430]7.0809,[431]7.0985,[432]7.1028,[433]7.1001,[434]7.0938,[435]7.0941,[436]7.0966,[437]7.1152,[438]7.1245,[439]7.1199,[440]7.1186,[441]7.1127,[442]7.1109,[443]7.1124,[444]7.1135,[445]7.1110,[446]7.1127,[447]7.1143,[448]7.1179,[449]7.1145,[450]7.1142,[451]7.1091,[452]7.1017,[453]7.0934,[454]7.0868,[455]7.0873,[456]7.0916,[457]7.0938,[458]7.0920,[459]7.0920,[460]7.1015,[461]7.0976,[462]7.0966,[463]7.1019,[464]7.1004,[465]7.0973,[466]7.0906,[467]7.0952,[468]7.0976,[469]7.1003,[470]7.1016,[471]7.0968,[472]7.1019,[473]7.0957,[474]7.0988,[475]7.0955,[476]7.0975,[477]7.0908,[478]7.0911,[479]7.1007,[480]7.1061,[481]7.1087,[482]7.1031,[483]7.0987,[484]7.1009,[485]7.1001,[486]7.0939,[487]7.0939,[488]7.0928,[489]7.0875,[490]7.0849,[491]7.0820,[492]7.0761,[493]7.0715,[494]7.0694,[495]7.0687,[496]7.0649,[497]7.0604,[498]7.0588,[499]7.0524,[500]7.0426,[501]7.0363,[502]7.0368,[503]7.0355,[504]7.0263,[505]7.0290,[506]7.0302,[507]7.0278,[508]7.0220,[509]7.0209,[510]7.0242,[511]7.0303,[512]7.0345,[513]7.0361,[514]7.0430,[515]7.0368,[516]7.0360,[517]7.0372,[518]7.0364,[519]7.0395,[520]7.0422,[521]7.0436,[522]7.0462,[523]7.0465,[524]7.0525,[525]7.0559,[526]7.0574,[527]7.0594,[528]7.0550,[529]7.0563,[530]7.0511,[531]7.0479,[532]7.0543,[533]7.0561,[534]7.0531,[535]7.0558,[536]7.0503,[537]7.0475,[538]7.0525,[539]7.0539,[540]7.0574,[541]7.0590,[542]7.0585,[543]7.0610,[544]7.0614,[545]7.0595,[546]7.0601,[547]7.0548,[548]7.0473,[549]7.0460,[550]7.0422,[551]7.0388,[552]7.0371,[553]7.0322,[554]7.0284,[555]7.0248,[556]7.0233,[557]7.0263,[558]7.0221,[559]7.0229,[560]7.0220,[561]7.0227,[562]7.0204,[563]7.0208,[564]7.0274,[565]7.0287,[566]7.0292,[567]7.0259,[568]7.0248,[569]7.0224,[570]7.0253,[571]7.0258,[572]7.0256,[573]7.0247,[574]7.0199,[575]7.0186,[576]7.0189,[577]7.0166,[578]7.0144,[579]7.0141,[580]7.0071,[581]7.0031,[582]7.0011,[583]7.0011,[584]7.0007,[585]6.9929,[586]6.9864,[587]6.9856,[588]6.9902,[589]6.9963,[590]6.9984,[591]6.9997,[592]6.9987,[593]6.9934,[594]6.9932,[595]6.9906,[596]6.9944,[597]6.9909,[598]6.9887,[599]6.9902,[600]6.9885,[601]6.9862,[602]6.9897,[603]6.9917,[604]6.9930,[605]6.9960,[606]6.9975,[607]6.9966,[608]6.9914,[609]6.9911,[610]6.9951,[611]6.9920,[612]6.9947,[613]6.9908,[614]6.9849,[615]6.9761,[616]6.9797,[617]6.9715,[618]6.9642,[619]6.9573,[620]6.9402,[621]6.9315,[622]6.9291,[623]6.9305,[624]6.9302,[625]6.9306,[626]6.9306,[627]6.9343,[628]6.9341,[629]6.9341,[630]6.9374,[631]6.9444,[632]6.9501,[633]6.9499,[634]6.9533,[635]6.9547,[636]6.9531,[637]6.9506,[638]6.9543,[639]6.9507,[640]6.9518,[641]6.9517,[642]6.9585,[643]6.9602,[644]6.9622,[645]6.9603,[646]6.9649,[647]6.9622,[648]6.9631,[649]6.9630,[650]6.9671,[651]6.9713,[652]6.9723,[653]6.9767,[654]6.9696,[655]6.9685,
llama_print_timings:        load time =     774.35 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =   72327.05 ms / 335360 tokens (    0.22 ms per token,  4636.72 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =   96847.60 ms / 335361 tokens

Final estimate: PPL = 6.9685 +/- 0.03974
