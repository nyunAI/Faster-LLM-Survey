main: build = 1833 (326b418b)
main: built with cc (Debian 10.2.1-6) 10.2.1 20210110 for x86_64-linux-gnu
main: seed  = 1706107512
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from exports/Llama-2-7b-hf/engine/llamacpp/ggml-model-f32-awq4g128.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 0
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - type  f32:  291 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = all F32
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 25.10 GiB (32.00 BPW) 
llm_load_print_meta: general.name     = LLaMA v2
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size       =    0.11 MiB
llm_load_tensors: using CUDA for GPU acceleration
llm_load_tensors: system memory used  =  500.11 MiB
llm_load_tensors: VRAM used           = 25205.02 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
....................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init: VRAM kv self = 256.00 MB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_build_graph: non-view tensors processed: 676/676
llama_new_context_with_model: compute buffer total size = 73.69 MiB
llama_new_context_with_model: VRAM scratch buffer: 70.50 MiB
llama_new_context_with_model: total VRAM used: 25531.52 MiB (model: 25205.02 MiB, context: 326.50 MiB)

system_info: n_threads = 32 / 12 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 1082.94 ms
perplexity: calculating perplexity over 655 chunks, batch_size=512
perplexity: 0.21 seconds per pass - ETA 2.32 minutes
[1]4.2065,[2]4.7417,[3]5.3832,[4]5.9615,[5]6.0841,[6]5.9941,[7]6.1667,[8]6.2544,[9]6.5731,[10]6.7518,[11]6.9669,[12]7.0126,[13]6.9365,[14]7.0094,[15]7.2306,[16]6.8902,[17]6.7737,[18]6.7616,[19]6.4430,[20]6.4385,[21]6.3615,[22]6.1923,[23]6.1606,[24]6.0709,[25]6.0626,[26]5.9073,[27]5.7250,[28]5.6278,[29]5.5470,[30]5.3978,[31]5.3609,[32]5.3818,[33]5.3370,[34]5.3645,[35]5.3792,[36]5.4034,[37]5.3976,[38]5.3961,[39]5.4101,[40]5.4602,[41]5.4825,[42]5.5190,[43]5.4820,[44]5.5375,[45]5.5469,[46]5.5237,[47]5.5476,[48]5.5263,[49]5.5262,[50]5.4932,[51]5.4936,[52]5.4840,[53]5.5306,[54]5.5184,[55]5.5032,[56]5.5326,[57]5.5515,[58]5.5801,[59]5.6013,[60]5.6512,[61]5.6482,[62]5.7093,[63]5.7441,[64]5.7508,[65]5.7935,[66]5.8040,[67]5.8226,[68]5.8428,[69]5.8769,[70]5.9145,[71]5.9406,[72]5.9754,[73]6.0273,[74]6.0374,[75]6.0485,[76]6.0659,[77]6.0811,[78]6.0709,[79]6.0976,[80]6.0963,[81]6.1110,[82]6.1162,[83]6.0679,[84]6.0572,[85]6.0534,[86]6.0365,[87]5.9787,[88]5.9549,[89]5.9402,[90]5.9324,[91]5.9524,[92]5.9514,[93]5.9539,[94]5.9541,[95]5.9827,[96]5.9814,[97]5.9737,[98]5.9681,[99]5.9573,[100]5.9592,[101]5.9820,[102]5.9777,[103]5.9946,[104]6.0039,[105]6.0056,[106]6.0222,[107]6.0246,[108]6.0395,[109]6.0382,[110]6.0345,[111]6.0539,[112]6.0736,[113]6.0760,[114]6.0766,[115]6.0850,[116]6.0736,[117]6.0776,[118]6.1022,[119]6.1225,[120]6.1558,[121]6.1718,[122]6.1937,[123]6.2336,[124]6.2507,[125]6.2422,[126]6.2774,[127]6.3123,[128]6.3370,[129]6.3209,[130]6.3299,[131]6.3242,[132]6.3153,[133]6.3012,[134]6.3087,[135]6.3064,[136]6.2949,[137]6.2893,[138]6.2718,[139]6.2647,[140]6.2615,[141]6.2367,[142]6.2320,[143]6.2043,[144]6.1848,[145]6.1760,[146]6.1662,[147]6.1731,[148]6.1743,[149]6.1673,[150]6.1674,[151]6.1734,[152]6.1665,[153]6.1533,[154]6.1460,[155]6.1534,[156]6.1513,[157]6.1666,[158]6.1695,[159]6.1714,[160]6.1738,[161]6.1864,[162]6.1589,[163]6.1479,[164]6.1229,[165]6.0923,[166]6.0647,[167]6.0271,[168]5.9964,[169]5.9825,[170]5.9711,[171]5.9464,[172]5.9326,[173]5.9172,[174]5.8890,[175]5.8682,[176]5.8552,[177]5.8355,[178]5.8139,[179]5.7982,[180]5.7892,[181]5.7708,[182]5.7529,[183]5.7386,[184]5.7368,[185]5.7288,[186]5.7307,[187]5.7354,[188]5.7339,[189]5.7494,[190]5.7496,[191]5.7675,[192]5.7853,[193]5.8027,[194]5.8162,[195]5.8387,[196]5.8528,[197]5.8721,[198]5.8870,[199]5.8896,[200]5.8934,[201]5.8862,[202]5.9022,[203]5.9109,[204]5.9086,[205]5.9198,[206]5.9235,[207]5.9218,[208]5.9309,[209]5.9335,[210]5.9396,[211]5.9494,[212]5.9552,[213]5.9649,[214]5.9662,[215]5.9691,[216]5.9828,[217]5.9988,[218]6.0126,[219]6.0126,[220]6.0098,[221]6.0025,[222]6.0001,[223]5.9914,[224]5.9819,[225]5.9769,[226]5.9968,[227]6.0013,[228]6.0069,[229]6.0124,[230]6.0081,[231]6.0224,[232]6.0109,[233]5.9954,[234]5.9794,[235]5.9559,[236]5.9497,[237]5.9378,[238]5.9399,[239]5.9265,[240]5.9157,[241]5.9183,[242]5.9200,[243]5.9161,[244]5.9049,[245]5.9012,[246]5.8900,[247]5.8791,[248]5.8718,[249]5.8680,[250]5.8713,[251]5.8630,[252]5.8589,[253]5.8493,[254]5.8438,[255]5.8336,[256]5.8167,[257]5.8054,[258]5.7972,[259]5.7975,[260]5.7895,[261]5.7838,[262]5.7782,[263]5.7723,[264]5.7484,[265]5.7478,[266]5.7444,[267]5.7379,[268]5.7460,[269]5.7448,[270]5.7459,[271]5.7522,[272]5.7553,[273]5.7557,[274]5.7572,[275]5.7638,[276]5.7701,[277]5.7857,[278]5.7945,[279]5.8046,[280]5.8076,[281]5.8182,[282]5.8230,[283]5.8380,[284]5.8474,[285]5.8567,[286]5.8702,[287]5.8697,[288]5.8758,[289]5.8680,[290]5.8523,[291]5.8380,[292]5.8220,[293]5.8082,[294]5.8079,[295]5.8073,[296]5.8115,[297]5.8094,[298]5.8108,[299]5.8070,[300]5.7958,[301]5.7950,[302]5.7873,[303]5.7796,[304]5.7707,[305]5.7669,[306]5.7544,[307]5.7561,[308]5.7570,[309]5.7412,[310]5.7371,[311]5.7313,[312]5.7317,[313]5.7256,[314]5.7223,[315]5.7066,[316]5.7015,[317]5.6867,[318]5.6672,[319]5.6803,[320]5.6933,[321]5.6986,[322]5.6946,[323]5.6889,[324]5.6880,[325]5.6993,[326]5.6997,[327]5.7017,[328]5.7053,[329]5.7110,[330]5.7136,[331]5.7255,[332]5.7216,[333]5.7291,[334]5.7227,[335]5.7167,[336]5.7186,[337]5.7170,[338]5.7171,[339]5.7125,[340]5.7084,[341]5.7156,[342]5.7174,[343]5.7219,[344]5.7214,[345]5.7217,[346]5.7190,[347]5.7214,[348]5.7248,[349]5.7276,[350]5.7257,[351]5.7261,[352]5.7262,[353]5.7201,[354]5.7188,[355]5.7227,[356]5.7256,[357]5.7233,[358]5.7317,[359]5.7342,[360]5.7319,[361]5.7317,[362]5.7388,[363]5.7501,[364]5.7562,[365]5.7612,[366]5.7636,[367]5.7716,[368]5.7687,[369]5.7700,[370]5.7716,[371]5.7669,[372]5.7724,[373]5.7765,[374]5.7752,[375]5.7740,[376]5.7812,[377]5.7778,[378]5.7808,[379]5.7855,[380]5.7780,[381]5.7752,[382]5.7702,[383]5.7683,[384]5.7673,[385]5.7663,[386]5.7670,[387]5.7671,[388]5.7622,[389]5.7578,[390]5.7517,[391]5.7450,[392]5.7403,[393]5.7404,[394]5.7428,[395]5.7409,[396]5.7332,[397]5.7404,[398]5.7444,[399]5.7515,[400]5.7505,[401]5.7525,[402]5.7534,[403]5.7554,[404]5.7614,[405]5.7526,[406]5.7489,[407]5.7484,[408]5.7507,[409]5.7618,[410]5.7725,[411]5.7818,[412]5.7967,[413]5.8079,[414]5.8140,[415]5.8198,[416]5.8268,[417]5.8386,[418]5.8426,[419]5.8478,[420]5.8559,[421]5.8664,[422]5.8703,[423]5.8759,[424]5.8856,[425]5.8937,[426]5.9003,[427]5.9037,[428]5.9110,[429]5.9161,[430]5.9230,[431]5.9371,[432]5.9406,[433]5.9391,[434]5.9354,[435]5.9368,[436]5.9396,[437]5.9490,[438]5.9562,[439]5.9524,[440]5.9503,[441]5.9456,[442]5.9447,[443]5.9462,[444]5.9470,[445]5.9452,[446]5.9472,[447]5.9493,[448]5.9529,[449]5.9506,[450]5.9509,[451]5.9473,[452]5.9329,[453]5.9233,[454]5.9170,[455]5.9175,[456]5.9223,[457]5.9239,[458]5.9220,[459]5.9219,[460]5.9299,[461]5.9262,[462]5.9246,[463]5.9278,[464]5.9269,[465]5.9248,[466]5.9178,[467]5.9192,[468]5.9193,[469]5.9212,[470]5.9220,[471]5.9182,[472]5.9220,[473]5.9161,[474]5.9172,[475]5.9109,[476]5.9129,[477]5.9058,[478]5.9050,[479]5.9102,[480]5.9149,[481]5.9173,[482]5.9133,[483]5.9095,[484]5.9111,[485]5.9096,[486]5.9053,[487]5.9045,[488]5.9024,[489]5.8982,[490]5.8965,[491]5.8939,[492]5.8887,[493]5.8854,[494]5.8834,[495]5.8813,[496]5.8778,[497]5.8731,[498]5.8714,[499]5.8671,[500]5.8585,[501]5.8530,[502]5.8531,[503]5.8514,[504]5.8430,[505]5.8448,[506]5.8453,[507]5.8400,[508]5.8357,[509]5.8357,[510]5.8379,[511]5.8426,[512]5.8460,[513]5.8477,[514]5.8536,[515]5.8487,[516]5.8483,[517]5.8486,[518]5.8480,[519]5.8505,[520]5.8527,[521]5.8538,[522]5.8564,[523]5.8567,[524]5.8618,[525]5.8650,[526]5.8663,[527]5.8676,[528]5.8628,[529]5.8630,[530]5.8587,[531]5.8567,[532]5.8617,[533]5.8637,[534]5.8627,[535]5.8659,[536]5.8606,[537]5.8592,[538]5.8637,[539]5.8649,[540]5.8668,[541]5.8675,[542]5.8682,[543]5.8704,[544]5.8715,[545]5.8702,[546]5.8713,[547]5.8673,[548]5.8624,[549]5.8618,[550]5.8594,[551]5.8570,[552]5.8555,[553]5.8519,[554]5.8495,[555]5.8466,[556]5.8461,[557]5.8484,[558]5.8448,[559]5.8453,[560]5.8453,[561]5.8456,[562]5.8432,[563]5.8430,[564]5.8476,[565]5.8487,[566]5.8492,[567]5.8465,[568]5.8468,[569]5.8454,[570]5.8483,[571]5.8490,[572]5.8496,[573]5.8494,[574]5.8457,[575]5.8443,[576]5.8442,[577]5.8426,[578]5.8412,[579]5.8413,[580]5.8359,[581]5.8331,[582]5.8323,[583]5.8336,[584]5.8338,[585]5.8266,[586]5.8201,[587]5.8199,[588]5.8239,[589]5.8290,[590]5.8319,[591]5.8338,[592]5.8324,[593]5.8284,[594]5.8289,[595]5.8270,[596]5.8308,[597]5.8284,[598]5.8252,[599]5.8270,[600]5.8261,[601]5.8247,[602]5.8245,[603]5.8261,[604]5.8275,[605]5.8307,[606]5.8325,[607]5.8314,[608]5.8274,[609]5.8283,[610]5.8322,[611]5.8310,[612]5.8332,[613]5.8305,[614]5.8259,[615]5.8195,[616]5.8221,[617]5.8164,[618]5.8113,[619]5.8064,[620]5.7935,[621]5.7872,[622]5.7851,[623]5.7870,[624]5.7873,[625]5.7882,[626]5.7877,[627]5.7905,[628]5.7913,[629]5.7917,[630]5.7949,[631]5.8001,[632]5.8058,[633]5.8053,[634]5.8082,[635]5.8094,[636]5.8065,[637]5.8027,[638]5.8051,[639]5.8013,[640]5.8025,[641]5.8028,[642]5.8086,[643]5.8102,[644]5.8111,[645]5.8098,[646]5.8135,[647]5.8096,[648]5.8104,[649]5.8110,[650]5.8144,[651]5.8187,[652]5.8195,[653]5.8230,[654]5.8169,[655]5.8160,
llama_print_timings:        load time =    7052.43 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =   74726.24 ms / 335360 tokens (    0.22 ms per token,  4487.85 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =   99149.87 ms / 335361 tokens

Final estimate: PPL = 5.8160 +/- 0.03249
