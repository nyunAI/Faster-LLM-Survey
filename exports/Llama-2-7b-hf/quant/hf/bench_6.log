You passed `quantization_config` to `from_pretrained` but the model you're loading already has a `quantization_config` attribute and has already quantized weights. However, loading attributes (e.g. ['use_cuda_fp16', 'use_exllama', 'max_input_length', 'exllama_config', 'disable_exllama']) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.
BASE: /home/shwu/LLM-Efficiency-Survey
MODEL: /home/shwu/LLM-Efficiency-Survey/model
MODEL_PATH: /home/shwu/LLM-Efficiency-Survey/model/Llama-2-7b-hf
METHOD_EXPORTS: /home/shwu/LLM-Efficiency-Survey/exports/Llama-2-7b-hf/quant/hf


=================================== Quant: gptq_3 ===================================
Output: What is the meaning of life?
 
### 1. 

* [1.1] 

* [1.2] 

* [1.3] 

* [1.4] 

* [1.5] 

* [1.6] 

* [1.7] 

* [1.8] 

* [1.9] 

* [1.10] 

* [1.11] 

* [1.12] 

* [1.13
======================================== Stats ========================================
Generated 128 tokens in 6.027063846588135 seconds (21.23753841971653 tokens/s)
================================================================================


