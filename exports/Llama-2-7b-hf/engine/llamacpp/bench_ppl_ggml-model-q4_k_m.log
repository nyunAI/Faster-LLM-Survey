main: build = 1833 (326b418b)
main: built with cc (Debian 10.2.1-6) 10.2.1 20210110 for x86_64-linux-gnu
main: seed  = 1706107301
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from exports/Llama-2-7b-hf/engine/llamacpp/ggml-model-q4_k_m.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 15
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) 
llm_load_print_meta: general.name     = LLaMA v2
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size       =    0.11 MiB
llm_load_tensors: using CUDA for GPU acceleration
llm_load_tensors: system memory used  =   70.42 MiB
llm_load_tensors: VRAM used           = 3820.93 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
..................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init: VRAM kv self = 256.00 MB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_build_graph: non-view tensors processed: 676/676
llama_new_context_with_model: compute buffer total size = 73.69 MiB
llama_new_context_with_model: VRAM scratch buffer: 70.50 MiB
llama_new_context_with_model: total VRAM used: 4147.43 MiB (model: 3820.93 MiB, context: 326.50 MiB)

system_info: n_threads = 32 / 12 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 1113.89 ms
perplexity: calculating perplexity over 655 chunks, batch_size=512
perplexity: 0.19 seconds per pass - ETA 2.10 minutes
[1]4.2763,[2]4.8013,[3]5.4460,[4]6.0140,[5]6.1219,[6]6.0326,[7]6.2121,[8]6.3015,[9]6.6350,[10]6.8222,[11]7.0444,[12]7.0933,[13]7.0107,[14]7.0775,[15]7.3112,[16]6.9703,[17]6.8508,[18]6.8345,[19]6.5126,[20]6.5018,[21]6.4196,[22]6.2456,[23]6.2105,[24]6.1182,[25]6.1130,[26]5.9643,[27]5.7832,[28]5.6865,[29]5.6044,[30]5.4540,[31]5.4181,[32]5.4370,[33]5.3897,[34]5.4184,[35]5.4327,[36]5.4568,[37]5.4537,[38]5.4497,[39]5.4661,[40]5.5192,[41]5.5406,[42]5.5768,[43]5.5391,[44]5.5953,[45]5.6057,[46]5.5812,[47]5.6059,[48]5.5848,[49]5.5839,[50]5.5504,[51]5.5507,[52]5.5408,[53]5.5861,[54]5.5732,[55]5.5595,[56]5.5895,[57]5.6092,[58]5.6365,[59]5.6594,[60]5.7091,[61]5.7055,[62]5.7676,[63]5.8036,[64]5.8103,[65]5.8552,[66]5.8664,[67]5.8867,[68]5.9071,[69]5.9413,[70]5.9768,[71]6.0047,[72]6.0396,[73]6.0934,[74]6.1035,[75]6.1139,[76]6.1308,[77]6.1449,[78]6.1354,[79]6.1618,[80]6.1606,[81]6.1768,[82]6.1835,[83]6.1353,[84]6.1274,[85]6.1228,[86]6.1057,[87]6.0502,[88]6.0263,[89]6.0118,[90]6.0025,[91]6.0237,[92]6.0229,[93]6.0241,[94]6.0241,[95]6.0532,[96]6.0524,[97]6.0453,[98]6.0405,[99]6.0296,[100]6.0314,[101]6.0551,[102]6.0506,[103]6.0687,[104]6.0774,[105]6.0793,[106]6.0964,[107]6.0989,[108]6.1139,[109]6.1120,[110]6.1079,[111]6.1273,[112]6.1482,[113]6.1506,[114]6.1507,[115]6.1588,[116]6.1471,[117]6.1526,[118]6.1783,[119]6.1991,[120]6.2341,[121]6.2508,[122]6.2731,[123]6.3135,[124]6.3314,[125]6.3229,[126]6.3588,[127]6.3943,[128]6.4196,[129]6.4035,[130]6.4122,[131]6.4055,[132]6.3962,[133]6.3816,[134]6.3891,[135]6.3865,[136]6.3744,[137]6.3674,[138]6.3493,[139]6.3427,[140]6.3393,[141]6.3138,[142]6.3087,[143]6.2813,[144]6.2615,[145]6.2526,[146]6.2422,[147]6.2491,[148]6.2512,[149]6.2439,[150]6.2441,[151]6.2502,[152]6.2422,[153]6.2279,[154]6.2205,[155]6.2273,[156]6.2254,[157]6.2397,[158]6.2421,[159]6.2438,[160]6.2460,[161]6.2589,[162]6.2308,[163]6.2190,[164]6.1941,[165]6.1632,[166]6.1351,[167]6.0970,[168]6.0656,[169]6.0515,[170]6.0410,[171]6.0160,[172]6.0019,[173]5.9864,[174]5.9582,[175]5.9371,[176]5.9236,[177]5.9036,[178]5.8822,[179]5.8662,[180]5.8574,[181]5.8387,[182]5.8209,[183]5.8067,[184]5.8046,[185]5.7970,[186]5.7990,[187]5.8041,[188]5.8027,[189]5.8183,[190]5.8183,[191]5.8359,[192]5.8535,[193]5.8717,[194]5.8855,[195]5.9075,[196]5.9215,[197]5.9408,[198]5.9555,[199]5.9583,[200]5.9620,[201]5.9544,[202]5.9712,[203]5.9797,[204]5.9780,[205]5.9894,[206]5.9931,[207]5.9907,[208]5.9995,[209]6.0029,[210]6.0089,[211]6.0193,[212]6.0251,[213]6.0346,[214]6.0361,[215]6.0385,[216]6.0526,[217]6.0689,[218]6.0830,[219]6.0825,[220]6.0797,[221]6.0723,[222]6.0699,[223]6.0615,[224]6.0525,[225]6.0478,[226]6.0680,[227]6.0720,[228]6.0775,[229]6.0827,[230]6.0779,[231]6.0928,[232]6.0805,[233]6.0647,[234]6.0490,[235]6.0262,[236]6.0200,[237]6.0082,[238]6.0103,[239]5.9963,[240]5.9853,[241]5.9876,[242]5.9897,[243]5.9858,[244]5.9748,[245]5.9701,[246]5.9586,[247]5.9472,[248]5.9397,[249]5.9358,[250]5.9394,[251]5.9312,[252]5.9265,[253]5.9169,[254]5.9107,[255]5.8999,[256]5.8827,[257]5.8710,[258]5.8625,[259]5.8627,[260]5.8541,[261]5.8484,[262]5.8427,[263]5.8368,[264]5.8130,[265]5.8123,[266]5.8093,[267]5.8030,[268]5.8116,[269]5.8109,[270]5.8117,[271]5.8175,[272]5.8207,[273]5.8205,[274]5.8217,[275]5.8282,[276]5.8340,[277]5.8490,[278]5.8581,[279]5.8683,[280]5.8709,[281]5.8817,[282]5.8866,[283]5.9014,[284]5.9111,[285]5.9202,[286]5.9333,[287]5.9329,[288]5.9397,[289]5.9320,[290]5.9161,[291]5.9020,[292]5.8861,[293]5.8724,[294]5.8720,[295]5.8709,[296]5.8755,[297]5.8733,[298]5.8751,[299]5.8712,[300]5.8598,[301]5.8588,[302]5.8511,[303]5.8431,[304]5.8339,[305]5.8297,[306]5.8172,[307]5.8185,[308]5.8195,[309]5.8036,[310]5.7993,[311]5.7934,[312]5.7934,[313]5.7875,[314]5.7843,[315]5.7685,[316]5.7638,[317]5.7492,[318]5.7292,[319]5.7428,[320]5.7560,[321]5.7610,[322]5.7567,[323]5.7513,[324]5.7506,[325]5.7620,[326]5.7625,[327]5.7644,[328]5.7686,[329]5.7745,[330]5.7769,[331]5.7888,[332]5.7850,[333]5.7925,[334]5.7860,[335]5.7796,[336]5.7813,[337]5.7798,[338]5.7798,[339]5.7752,[340]5.7712,[341]5.7782,[342]5.7800,[343]5.7844,[344]5.7839,[345]5.7838,[346]5.7811,[347]5.7837,[348]5.7871,[349]5.7902,[350]5.7880,[351]5.7885,[352]5.7887,[353]5.7826,[354]5.7821,[355]5.7859,[356]5.7885,[357]5.7859,[358]5.7945,[359]5.7971,[360]5.7944,[361]5.7940,[362]5.8011,[363]5.8124,[364]5.8186,[365]5.8238,[366]5.8262,[367]5.8344,[368]5.8318,[369]5.8334,[370]5.8350,[371]5.8299,[372]5.8356,[373]5.8398,[374]5.8386,[375]5.8375,[376]5.8449,[377]5.8415,[378]5.8446,[379]5.8495,[380]5.8422,[381]5.8391,[382]5.8341,[383]5.8322,[384]5.8311,[385]5.8296,[386]5.8303,[387]5.8303,[388]5.8250,[389]5.8205,[390]5.8142,[391]5.8075,[392]5.8029,[393]5.8030,[394]5.8054,[395]5.8035,[396]5.7960,[397]5.8032,[398]5.8073,[399]5.8145,[400]5.8135,[401]5.8158,[402]5.8166,[403]5.8187,[404]5.8248,[405]5.8162,[406]5.8123,[407]5.8118,[408]5.8140,[409]5.8253,[410]5.8362,[411]5.8455,[412]5.8607,[413]5.8718,[414]5.8782,[415]5.8840,[416]5.8911,[417]5.9033,[418]5.9071,[419]5.9121,[420]5.9205,[421]5.9311,[422]5.9348,[423]5.9405,[424]5.9501,[425]5.9585,[426]5.9651,[427]5.9686,[428]5.9759,[429]5.9809,[430]5.9879,[431]6.0020,[432]6.0053,[433]6.0038,[434]5.9997,[435]6.0009,[436]6.0036,[437]6.0130,[438]6.0202,[439]6.0164,[440]6.0143,[441]6.0097,[442]6.0086,[443]6.0102,[444]6.0109,[445]6.0091,[446]6.0111,[447]6.0133,[448]6.0171,[449]6.0147,[450]6.0148,[451]6.0111,[452]5.9970,[453]5.9876,[454]5.9814,[455]5.9816,[456]5.9862,[457]5.9878,[458]5.9859,[459]5.9858,[460]5.9937,[461]5.9902,[462]5.9886,[463]5.9923,[464]5.9914,[465]5.9893,[466]5.9822,[467]5.9837,[468]5.9838,[469]5.9857,[470]5.9863,[471]5.9826,[472]5.9865,[473]5.9808,[474]5.9824,[475]5.9760,[476]5.9780,[477]5.9711,[478]5.9703,[479]5.9759,[480]5.9809,[481]5.9832,[482]5.9794,[483]5.9755,[484]5.9771,[485]5.9756,[486]5.9711,[487]5.9705,[488]5.9685,[489]5.9642,[490]5.9624,[491]5.9599,[492]5.9547,[493]5.9512,[494]5.9493,[495]5.9473,[496]5.9435,[497]5.9388,[498]5.9372,[499]5.9328,[500]5.9241,[501]5.9187,[502]5.9188,[503]5.9170,[504]5.9085,[505]5.9104,[506]5.9110,[507]5.9056,[508]5.9012,[509]5.9010,[510]5.9030,[511]5.9076,[512]5.9111,[513]5.9128,[514]5.9186,[515]5.9135,[516]5.9131,[517]5.9133,[518]5.9130,[519]5.9153,[520]5.9176,[521]5.9186,[522]5.9211,[523]5.9217,[524]5.9269,[525]5.9301,[526]5.9314,[527]5.9328,[528]5.9281,[529]5.9287,[530]5.9240,[531]5.9219,[532]5.9270,[533]5.9289,[534]5.9280,[535]5.9310,[536]5.9256,[537]5.9243,[538]5.9287,[539]5.9299,[540]5.9319,[541]5.9329,[542]5.9333,[543]5.9356,[544]5.9368,[545]5.9355,[546]5.9365,[547]5.9326,[548]5.9275,[549]5.9271,[550]5.9246,[551]5.9221,[552]5.9208,[553]5.9171,[554]5.9148,[555]5.9116,[556]5.9112,[557]5.9136,[558]5.9102,[559]5.9106,[560]5.9105,[561]5.9109,[562]5.9082,[563]5.9079,[564]5.9125,[565]5.9138,[566]5.9144,[567]5.9116,[568]5.9115,[569]5.9100,[570]5.9129,[571]5.9137,[572]5.9144,[573]5.9143,[574]5.9106,[575]5.9092,[576]5.9089,[577]5.9071,[578]5.9055,[579]5.9056,[580]5.9002,[581]5.8973,[582]5.8965,[583]5.8978,[584]5.8980,[585]5.8907,[586]5.8843,[587]5.8841,[588]5.8880,[589]5.8932,[590]5.8959,[591]5.8977,[592]5.8963,[593]5.8925,[594]5.8930,[595]5.8910,[596]5.8947,[597]5.8922,[598]5.8892,[599]5.8911,[600]5.8901,[601]5.8887,[602]5.8887,[603]5.8903,[604]5.8919,[605]5.8953,[606]5.8971,[607]5.8963,[608]5.8922,[609]5.8930,[610]5.8970,[611]5.8955,[612]5.8979,[613]5.8951,[614]5.8905,[615]5.8839,[616]5.8868,[617]5.8808,[618]5.8758,[619]5.8707,[620]5.8575,[621]5.8511,[622]5.8489,[623]5.8506,[624]5.8508,[625]5.8518,[626]5.8512,[627]5.8540,[628]5.8547,[629]5.8551,[630]5.8584,[631]5.8637,[632]5.8693,[633]5.8689,[634]5.8719,[635]5.8731,[636]5.8704,[637]5.8667,[638]5.8693,[639]5.8656,[640]5.8668,[641]5.8671,[642]5.8728,[643]5.8744,[644]5.8755,[645]5.8740,[646]5.8777,[647]5.8740,[648]5.8749,[649]5.8755,[650]5.8788,[651]5.8832,[652]5.8841,[653]5.8875,[654]5.8814,[655]5.8804,
llama_print_timings:        load time =    1171.42 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =   71809.07 ms / 335360 tokens (    0.21 ms per token,  4670.16 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =   96829.39 ms / 335361 tokens

Final estimate: PPL = 5.8804 +/- 0.03280
