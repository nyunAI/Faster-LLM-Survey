[2m2024-01-20T17:25:53.525438Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Args { model_id: "/home/shwu/LLM-Efficiency-Survey/model/Llama-2-7b-hf", revision: None, validation_workers: 2, sharded: None, num_shard: None, quantize: Some(Eetq), speculate: None, dtype: None, trust_remote_code: false, max_concurrent_requests: 128, max_best_of: 2, max_stop_sequences: 4, max_top_n_tokens: 5, max_input_length: 1024, max_total_tokens: 2048, waiting_served_ratio: 1.2, max_batch_prefill_tokens: 4096, max_batch_total_tokens: None, max_waiting_tokens: 20, hostname: "0.0.0.0", port: 3000, shard_uds_path: "/tmp/text-generation-server", master_addr: "localhost", master_port: 29500, huggingface_hub_cache: None, weights_cache_override: None, disable_custom_kernels: false, cuda_memory_fraction: 1.0, rope_scaling: None, rope_factor: None, json_output: false, otlp_endpoint: None, cors_allow_origin: [], watermark_gamma: None, watermark_delta: None, ngrok: false, ngrok_authtoken: None, ngrok_edge: None, env: false }
[2m2024-01-20T17:25:53.525846Z[0m [32m INFO[0m [1mdownload[0m: [2mtext_generation_launcher[0m[2m:[0m Starting download process.
[2m2024-01-20T17:25:56.087971Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Files are already present on the host. Skipping download.

[2m2024-01-20T17:25:56.629435Z[0m [32m INFO[0m [1mdownload[0m: [2mtext_generation_launcher[0m[2m:[0m Successfully downloaded weights.
[2m2024-01-20T17:25:56.629703Z[0m [32m INFO[0m [1mshard-manager[0m: [2mtext_generation_launcher[0m[2m:[0m Starting shard [2m[3mrank[0m[2m=[0m0[0m
[2m2024-01-20T17:26:00.464555Z[0m [33m WARN[0m [2mtext_generation_launcher[0m[2m:[0m Could not import Flash Attention enabled models: No module named 'vllm'

[2m2024-01-20T17:26:05.726624Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Server started at unix:///tmp/text-generation-server-0

[2m2024-01-20T17:26:05.740957Z[0m [32m INFO[0m [1mshard-manager[0m: [2mtext_generation_launcher[0m[2m:[0m Shard ready in 9.110575213s [2m[3mrank[0m[2m=[0m0[0m
[2m2024-01-20T17:26:05.838681Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Starting Webserver
[2m2024-01-20T17:26:05.926295Z[0m [33m WARN[0m [2mtext_generation_router[0m[2m:[0m [2mrouter/src/main.rs[0m[2m:[0m[2m246:[0m Could not find tokenizer config locally and no revision specified
[2m2024-01-20T17:26:05.926367Z[0m [33m WARN[0m [2mtext_generation_router[0m[2m:[0m [2mrouter/src/main.rs[0m[2m:[0m[2m258:[0m no pipeline tag found for model /home/shwu/LLM-Efficiency-Survey/model/Llama-2-7b-hf
[2m2024-01-20T17:26:05.932633Z[0m [32m INFO[0m [2mtext_generation_router[0m[2m:[0m [2mrouter/src/main.rs[0m[2m:[0m[2m277:[0m Warming up model
[2m2024-01-20T17:26:07.349426Z[0m [33m WARN[0m [2mtext_generation_router[0m[2m:[0m [2mrouter/src/main.rs[0m[2m:[0m[2m291:[0m Model does not support automatic max batch total tokens
[2m2024-01-20T17:26:07.349449Z[0m [32m INFO[0m [2mtext_generation_router[0m[2m:[0m [2mrouter/src/main.rs[0m[2m:[0m[2m313:[0m Setting max batch total tokens to 16000
[2m2024-01-20T17:26:07.349453Z[0m [32m INFO[0m [2mtext_generation_router[0m[2m:[0m [2mrouter/src/main.rs[0m[2m:[0m[2m314:[0m Connected
[2m2024-01-20T17:26:23.604158Z[0m [32m INFO[0m [1mgenerate[0m[1m{[0m[3mparameters[0m[2m=[0mGenerateParameters { best_of: None, temperature: None, repetition_penalty: None, top_k: None, top_p: None, typical_p: None, do_sample: false, max_new_tokens: Some(128), return_full_text: None, stop: [], truncate: None, watermark: false, details: false, decoder_input_details: false, seed: None, top_n_tokens: None } [3mtotal_time[0m[2m=[0m"4.156001932s" [3mvalidation_time[0m[2m=[0m"682.507Âµs" [3mqueue_time[0m[2m=[0m"112.433Âµs" [3minference_time[0m[2m=[0m"4.155207322s" [3mtime_per_token[0m[2m=[0m"32.462557ms" [3mseed[0m[2m=[0m"None"[1m}[0m[2m:[0m [2mtext_generation_router::server[0m[2m:[0m [2mrouter/src/server.rs[0m[2m:[0m[2m296:[0m Success
[2m2024-01-20T17:26:27.471076Z[0m [32m INFO[0m [1mgenerate[0m[1m{[0m[3mparameters[0m[2m=[0mGenerateParameters { best_of: None, temperature: None, repetition_penalty: None, top_k: None, top_p: None, typical_p: None, do_sample: false, max_new_tokens: Some(128), return_full_text: None, stop: [], truncate: None, watermark: false, details: false, decoder_input_details: false, seed: None, top_n_tokens: None } [3mtotal_time[0m[2m=[0m"3.864164981s" [3mvalidation_time[0m[2m=[0m"385.2Âµs" [3mqueue_time[0m[2m=[0m"56.318Âµs" [3minference_time[0m[2m=[0m"3.863723739s" [3mtime_per_token[0m[2m=[0m"30.185341ms" [3mseed[0m[2m=[0m"None"[1m}[0m[2m:[0m [2mtext_generation_router::server[0m[2m:[0m [2mrouter/src/server.rs[0m[2m:[0m[2m296:[0m Success
[2m2024-01-20T17:26:31.322324Z[0m [32m INFO[0m [1mgenerate[0m[1m{[0m[3mparameters[0m[2m=[0mGenerateParameters { best_of: None, temperature: None, repetition_penalty: None, top_k: None, top_p: None, typical_p: None, do_sample: false, max_new_tokens: Some(128), return_full_text: None, stop: [], truncate: None, watermark: false, details: false, decoder_input_details: false, seed: None, top_n_tokens: None } [3mtotal_time[0m[2m=[0m"3.848710499s" [3mvalidation_time[0m[2m=[0m"214.962Âµs" [3mqueue_time[0m[2m=[0m"53.957Âµs" [3minference_time[0m[2m=[0m"3.848441785s" [3mtime_per_token[0m[2m=[0m"30.065951ms" [3mseed[0m[2m=[0m"None"[1m}[0m[2m:[0m [2mtext_generation_router::server[0m[2m:[0m [2mrouter/src/server.rs[0m[2m:[0m[2m296:[0m Success
[2m2024-01-20T17:26:35.195563Z[0m [32m INFO[0m [1mgenerate[0m[1m{[0m[3mparameters[0m[2m=[0mGenerateParameters { best_of: None, temperature: None, repetition_penalty: None, top_k: None, top_p: None, typical_p: None, do_sample: false, max_new_tokens: Some(128), return_full_text: None, stop: [], truncate: None, watermark: false, details: false, decoder_input_details: false, seed: None, top_n_tokens: None } [3mtotal_time[0m[2m=[0m"3.870528849s" [3mvalidation_time[0m[2m=[0m"248.779Âµs" [3mqueue_time[0m[2m=[0m"60.187Âµs" [3minference_time[0m[2m=[0m"3.870220053s" [3mtime_per_token[0m[2m=[0m"30.236094ms" [3mseed[0m[2m=[0m"None"[1m}[0m[2m:[0m [2mtext_generation_router::server[0m[2m:[0m [2mrouter/src/server.rs[0m[2m:[0m[2m296:[0m Success
[2m2024-01-20T17:26:39.098025Z[0m [32m INFO[0m [1mgenerate[0m[1m{[0m[3mparameters[0m[2m=[0mGenerateParameters { best_of: None, temperature: None, repetition_penalty: None, top_k: None, top_p: None, typical_p: None, do_sample: false, max_new_tokens: Some(128), return_full_text: None, stop: [], truncate: None, watermark: false, details: false, decoder_input_details: false, seed: None, top_n_tokens: None } [3mtotal_time[0m[2m=[0m"3.899769297s" [3mvalidation_time[0m[2m=[0m"285.825Âµs" [3mqueue_time[0m[2m=[0m"63.663Âµs" [3minference_time[0m[2m=[0m"3.899420049s" [3mtime_per_token[0m[2m=[0m"30.464219ms" [3mseed[0m[2m=[0m"None"[1m}[0m[2m:[0m [2mtext_generation_router::server[0m[2m:[0m [2mrouter/src/server.rs[0m[2m:[0m[2m296:[0m Success
[2m2024-01-20T17:26:42.970285Z[0m [32m INFO[0m [1mgenerate[0m[1m{[0m[3mparameters[0m[2m=[0mGenerateParameters { best_of: None, temperature: None, repetition_penalty: None, top_k: None, top_p: None, typical_p: None, do_sample: false, max_new_tokens: Some(128), return_full_text: None, stop: [], truncate: None, watermark: false, details: false, decoder_input_details: false, seed: None, top_n_tokens: None } [3mtotal_time[0m[2m=[0m"3.869685567s" [3mvalidation_time[0m[2m=[0m"356.151Âµs" [3mqueue_time[0m[2m=[0m"64.62Âµs" [3minference_time[0m[2m=[0m"3.869265084s" [3mtime_per_token[0m[2m=[0m"30.228633ms" [3mseed[0m[2m=[0m"None"[1m}[0m[2m:[0m [2mtext_generation_router::server[0m[2m:[0m [2mrouter/src/server.rs[0m[2m:[0m[2m296:[0m Success
[2m2024-01-20T17:26:46.798506Z[0m [32m INFO[0m [1mgenerate[0m[1m{[0m[3mparameters[0m[2m=[0mGenerateParameters { best_of: None, temperature: None, repetition_penalty: None, top_k: None, top_p: None, typical_p: None, do_sample: false, max_new_tokens: Some(128), return_full_text: None, stop: [], truncate: None, watermark: false, details: false, decoder_input_details: false, seed: None, top_n_tokens: None } [3mtotal_time[0m[2m=[0m"3.825722251s" [3mvalidation_time[0m[2m=[0m"195.684Âµs" [3mqueue_time[0m[2m=[0m"39.61Âµs" [3minference_time[0m[2m=[0m"3.82548718s" [3mtime_per_token[0m[2m=[0m"29.886618ms" [3mseed[0m[2m=[0m"None"[1m}[0m[2m:[0m [2mtext_generation_router::server[0m[2m:[0m [2mrouter/src/server.rs[0m[2m:[0m[2m296:[0m Success
[2m2024-01-20T17:26:50.734640Z[0m [32m INFO[0m [1mgenerate[0m[1m{[0m[3mparameters[0m[2m=[0mGenerateParameters { best_of: None, temperature: None, repetition_penalty: None, top_k: None, top_p: None, typical_p: None, do_sample: false, max_new_tokens: Some(128), return_full_text: None, stop: [], truncate: None, watermark: false, details: false, decoder_input_details: false, seed: None, top_n_tokens: None } [3mtotal_time[0m[2m=[0m"3.933316762s" [3mvalidation_time[0m[2m=[0m"263.606Âµs" [3mqueue_time[0m[2m=[0m"52.41Âµs" [3minference_time[0m[2m=[0m"3.933000947s" [3mtime_per_token[0m[2m=[0m"30.726569ms" [3mseed[0m[2m=[0m"None"[1m}[0m[2m:[0m [2mtext_generation_router::server[0m[2m:[0m [2mrouter/src/server.rs[0m[2m:[0m[2m296:[0m Success
[2m2024-01-20T17:26:54.635750Z[0m [32m INFO[0m [1mgenerate[0m[1m{[0m[3mparameters[0m[2m=[0mGenerateParameters { best_of: None, temperature: None, repetition_penalty: None, top_k: None, top_p: None, typical_p: None, do_sample: false, max_new_tokens: Some(128), return_full_text: None, stop: [], truncate: None, watermark: false, details: false, decoder_input_details: false, seed: None, top_n_tokens: None } [3mtotal_time[0m[2m=[0m"3.898461302s" [3mvalidation_time[0m[2m=[0m"243.365Âµs" [3mqueue_time[0m[2m=[0m"49.435Âµs" [3minference_time[0m[2m=[0m"3.898168807s" [3mtime_per_token[0m[2m=[0m"30.454443ms" [3mseed[0m[2m=[0m"None"[1m}[0m[2m:[0m [2mtext_generation_router::server[0m[2m:[0m [2mrouter/src/server.rs[0m[2m:[0m[2m296:[0m Success
[2m2024-01-20T17:26:58.531705Z[0m [32m INFO[0m [1mgenerate[0m[1m{[0m[3mparameters[0m[2m=[0mGenerateParameters { best_of: None, temperature: None, repetition_penalty: None, top_k: None, top_p: None, typical_p: None, do_sample: false, max_new_tokens: Some(128), return_full_text: None, stop: [], truncate: None, watermark: false, details: false, decoder_input_details: false, seed: None, top_n_tokens: None } [3mtotal_time[0m[2m=[0m"3.893248719s" [3mvalidation_time[0m[2m=[0m"244.792Âµs" [3mqueue_time[0m[2m=[0m"46.753Âµs" [3minference_time[0m[2m=[0m"3.892957403s" [3mtime_per_token[0m[2m=[0m"30.413729ms" [3mseed[0m[2m=[0m"None"[1m}[0m[2m:[0m [2mtext_generation_router::server[0m[2m:[0m [2mrouter/src/server.rs[0m[2m:[0m[2m296:[0m Success
[2m2024-01-20T17:26:58.537568Z[0m [32m INFO[0m [2mtext_generation_router::server[0m[2m:[0m [2mrouter/src/server.rs[0m[2m:[0m[2m974:[0m signal received, starting graceful shutdown
[2m2024-01-20T17:26:58.597274Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Terminating webserver
[2m2024-01-20T17:26:58.597332Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Waiting for webserver to gracefully shutdown
[2m2024-01-20T17:26:58.597355Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m webserver terminated
[2m2024-01-20T17:26:58.597359Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Shutting down shards
[2m2024-01-20T17:26:58.680655Z[0m [32m INFO[0m [1mshard-manager[0m: [2mtext_generation_launcher[0m[2m:[0m Shard terminated [2m[3mrank[0m[2m=[0m0[0m
