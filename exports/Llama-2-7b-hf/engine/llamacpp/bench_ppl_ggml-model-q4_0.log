main: build = 1833 (326b418b)
main: built with cc (Debian 10.2.1-6) 10.2.1 20210110 for x86_64-linux-gnu
main: seed  = 1706106988
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from exports/Llama-2-7b-hf/engine/llamacpp/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 2
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = LLaMA v2
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size       =    0.11 MiB
llm_load_tensors: using CUDA for GPU acceleration
llm_load_tensors: system memory used  =   70.42 MiB
llm_load_tensors: VRAM used           = 3577.55 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
..................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init: VRAM kv self = 256.00 MB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_build_graph: non-view tensors processed: 676/676
llama_new_context_with_model: compute buffer total size = 73.69 MiB
llama_new_context_with_model: VRAM scratch buffer: 70.50 MiB
llama_new_context_with_model: total VRAM used: 3904.05 MiB (model: 3577.55 MiB, context: 326.50 MiB)

system_info: n_threads = 32 / 12 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 1090.36 ms
perplexity: calculating perplexity over 655 chunks, batch_size=512
perplexity: 0.19 seconds per pass - ETA 2.07 minutes
[1]4.3453,[2]4.8369,[3]5.4664,[4]6.0701,[5]6.2000,[6]6.0968,[7]6.2719,[8]6.3808,[9]6.7400,[10]6.9489,[11]7.1688,[12]7.2011,[13]7.1422,[14]7.2244,[15]7.4424,[16]7.0993,[17]6.9772,[18]6.9645,[19]6.6329,[20]6.6165,[21]6.5246,[22]6.3449,[23]6.3143,[24]6.2159,[25]6.1978,[26]6.0398,[27]5.8505,[28]5.7467,[29]5.6688,[30]5.5158,[31]5.4759,[32]5.4992,[33]5.4581,[34]5.4834,[35]5.4998,[36]5.5277,[37]5.5264,[38]5.5312,[39]5.5475,[40]5.5990,[41]5.6176,[42]5.6574,[43]5.6160,[44]5.6656,[45]5.6820,[46]5.6599,[47]5.6825,[48]5.6614,[49]5.6609,[50]5.6276,[51]5.6266,[52]5.6158,[53]5.6644,[54]5.6514,[55]5.6373,[56]5.6670,[57]5.6864,[58]5.7178,[59]5.7393,[60]5.7901,[61]5.7855,[62]5.8498,[63]5.8860,[64]5.8933,[65]5.9387,[66]5.9488,[67]5.9672,[68]5.9878,[69]6.0254,[70]6.0642,[71]6.0901,[72]6.1263,[73]6.1804,[74]6.1915,[75]6.2033,[76]6.2188,[77]6.2342,[78]6.2240,[79]6.2521,[80]6.2504,[81]6.2634,[82]6.2681,[83]6.2193,[84]6.2097,[85]6.2051,[86]6.1863,[87]6.1329,[88]6.1084,[89]6.0909,[90]6.0829,[91]6.1044,[92]6.1029,[93]6.1043,[94]6.1051,[95]6.1344,[96]6.1323,[97]6.1247,[98]6.1196,[99]6.1079,[100]6.1112,[101]6.1339,[102]6.1302,[103]6.1476,[104]6.1555,[105]6.1582,[106]6.1752,[107]6.1778,[108]6.1940,[109]6.1922,[110]6.1870,[111]6.2068,[112]6.2283,[113]6.2295,[114]6.2293,[115]6.2378,[116]6.2251,[117]6.2288,[118]6.2540,[119]6.2742,[120]6.3100,[121]6.3264,[122]6.3499,[123]6.3917,[124]6.4092,[125]6.3998,[126]6.4349,[127]6.4724,[128]6.4969,[129]6.4803,[130]6.4899,[131]6.4845,[132]6.4750,[133]6.4622,[134]6.4688,[135]6.4651,[136]6.4541,[137]6.4472,[138]6.4297,[139]6.4223,[140]6.4190,[141]6.3942,[142]6.3901,[143]6.3622,[144]6.3428,[145]6.3361,[146]6.3269,[147]6.3359,[148]6.3377,[149]6.3301,[150]6.3293,[151]6.3342,[152]6.3261,[153]6.3122,[154]6.3055,[155]6.3126,[156]6.3102,[157]6.3262,[158]6.3283,[159]6.3297,[160]6.3330,[161]6.3455,[162]6.3175,[163]6.3053,[164]6.2800,[165]6.2492,[166]6.2209,[167]6.1820,[168]6.1498,[169]6.1347,[170]6.1222,[171]6.0971,[172]6.0823,[173]6.0664,[174]6.0379,[175]6.0159,[176]6.0030,[177]5.9821,[178]5.9596,[179]5.9431,[180]5.9341,[181]5.9145,[182]5.8969,[183]5.8827,[184]5.8801,[185]5.8716,[186]5.8743,[187]5.8791,[188]5.8783,[189]5.8934,[190]5.8938,[191]5.9121,[192]5.9303,[193]5.9489,[194]5.9628,[195]5.9857,[196]6.0012,[197]6.0204,[198]6.0354,[199]6.0386,[200]6.0423,[201]6.0352,[202]6.0522,[203]6.0612,[204]6.0583,[205]6.0692,[206]6.0736,[207]6.0715,[208]6.0814,[209]6.0833,[210]6.0895,[211]6.0993,[212]6.1050,[213]6.1145,[214]6.1163,[215]6.1202,[216]6.1340,[217]6.1504,[218]6.1656,[219]6.1652,[220]6.1620,[221]6.1536,[222]6.1506,[223]6.1420,[224]6.1318,[225]6.1266,[226]6.1470,[227]6.1523,[228]6.1586,[229]6.1639,[230]6.1600,[231]6.1748,[232]6.1633,[233]6.1473,[234]6.1312,[235]6.1084,[236]6.1021,[237]6.0896,[238]6.0928,[239]6.0791,[240]6.0686,[241]6.0708,[242]6.0726,[243]6.0684,[244]6.0575,[245]6.0542,[246]6.0422,[247]6.0309,[248]6.0227,[249]6.0193,[250]6.0228,[251]6.0144,[252]6.0104,[253]6.0008,[254]5.9959,[255]5.9852,[256]5.9679,[257]5.9566,[258]5.9483,[259]5.9483,[260]5.9392,[261]5.9337,[262]5.9278,[263]5.9227,[264]5.8996,[265]5.8981,[266]5.8951,[267]5.8885,[268]5.8966,[269]5.8956,[270]5.8971,[271]5.9031,[272]5.9068,[273]5.9075,[274]5.9087,[275]5.9159,[276]5.9226,[277]5.9391,[278]5.9481,[279]5.9574,[280]5.9599,[281]5.9701,[282]5.9750,[283]5.9907,[284]6.0005,[285]6.0100,[286]6.0242,[287]6.0235,[288]6.0298,[289]6.0216,[290]6.0054,[291]5.9908,[292]5.9743,[293]5.9603,[294]5.9603,[295]5.9597,[296]5.9646,[297]5.9626,[298]5.9642,[299]5.9604,[300]5.9490,[301]5.9476,[302]5.9402,[303]5.9330,[304]5.9242,[305]5.9203,[306]5.9078,[307]5.9100,[308]5.9114,[309]5.8954,[310]5.8909,[311]5.8850,[312]5.8851,[313]5.8790,[314]5.8757,[315]5.8598,[316]5.8558,[317]5.8404,[318]5.8202,[319]5.8334,[320]5.8463,[321]5.8511,[322]5.8467,[323]5.8416,[324]5.8405,[325]5.8517,[326]5.8524,[327]5.8546,[328]5.8589,[329]5.8654,[330]5.8681,[331]5.8803,[332]5.8765,[333]5.8841,[334]5.8775,[335]5.8711,[336]5.8728,[337]5.8711,[338]5.8705,[339]5.8653,[340]5.8613,[341]5.8680,[342]5.8697,[343]5.8745,[344]5.8742,[345]5.8743,[346]5.8713,[347]5.8735,[348]5.8776,[349]5.8799,[350]5.8776,[351]5.8781,[352]5.8783,[353]5.8722,[354]5.8712,[355]5.8753,[356]5.8780,[357]5.8752,[358]5.8842,[359]5.8868,[360]5.8842,[361]5.8835,[362]5.8907,[363]5.9022,[364]5.9082,[365]5.9131,[366]5.9153,[367]5.9236,[368]5.9207,[369]5.9220,[370]5.9238,[371]5.9184,[372]5.9239,[373]5.9285,[374]5.9273,[375]5.9257,[376]5.9335,[377]5.9297,[378]5.9331,[379]5.9375,[380]5.9296,[381]5.9264,[382]5.9211,[383]5.9190,[384]5.9179,[385]5.9168,[386]5.9171,[387]5.9172,[388]5.9118,[389]5.9072,[390]5.9008,[391]5.8936,[392]5.8890,[393]5.8893,[394]5.8920,[395]5.8900,[396]5.8818,[397]5.8893,[398]5.8934,[399]5.9007,[400]5.8998,[401]5.9015,[402]5.9019,[403]5.9040,[404]5.9098,[405]5.9014,[406]5.8975,[407]5.8976,[408]5.8997,[409]5.9108,[410]5.9217,[411]5.9319,[412]5.9472,[413]5.9584,[414]5.9645,[415]5.9706,[416]5.9781,[417]5.9907,[418]5.9945,[419]5.9998,[420]6.0081,[421]6.0188,[422]6.0234,[423]6.0289,[424]6.0386,[425]6.0470,[426]6.0539,[427]6.0576,[428]6.0652,[429]6.0704,[430]6.0773,[431]6.0918,[432]6.0958,[433]6.0941,[434]6.0897,[435]6.0911,[436]6.0938,[437]6.1034,[438]6.1107,[439]6.1068,[440]6.1051,[441]6.1003,[442]6.0994,[443]6.1009,[444]6.1018,[445]6.1002,[446]6.1021,[447]6.1043,[448]6.1081,[449]6.1057,[450]6.1058,[451]6.1019,[452]6.0882,[453]6.0789,[454]6.0729,[455]6.0734,[456]6.0783,[457]6.0799,[458]6.0784,[459]6.0781,[460]6.0862,[461]6.0822,[462]6.0806,[463]6.0844,[464]6.0833,[465]6.0814,[466]6.0744,[467]6.0757,[468]6.0756,[469]6.0777,[470]6.0782,[471]6.0743,[472]6.0783,[473]6.0725,[474]6.0740,[475]6.0679,[476]6.0698,[477]6.0624,[478]6.0614,[479]6.0673,[480]6.0724,[481]6.0745,[482]6.0704,[483]6.0661,[484]6.0678,[485]6.0664,[486]6.0617,[487]6.0611,[488]6.0589,[489]6.0544,[490]6.0529,[491]6.0501,[492]6.0447,[493]6.0413,[494]6.0392,[495]6.0370,[496]6.0333,[497]6.0283,[498]6.0265,[499]6.0225,[500]6.0138,[501]6.0079,[502]6.0080,[503]6.0062,[504]5.9977,[505]5.9994,[506]5.9999,[507]5.9945,[508]5.9901,[509]5.9901,[510]5.9922,[511]5.9969,[512]6.0009,[513]6.0023,[514]6.0079,[515]6.0025,[516]6.0023,[517]6.0025,[518]6.0018,[519]6.0040,[520]6.0064,[521]6.0076,[522]6.0101,[523]6.0105,[524]6.0159,[525]6.0194,[526]6.0207,[527]6.0221,[528]6.0172,[529]6.0173,[530]6.0127,[531]6.0106,[532]6.0153,[533]6.0175,[534]6.0164,[535]6.0197,[536]6.0144,[537]6.0128,[538]6.0173,[539]6.0188,[540]6.0210,[541]6.0223,[542]6.0229,[543]6.0248,[544]6.0258,[545]6.0245,[546]6.0259,[547]6.0218,[548]6.0169,[549]6.0161,[550]6.0132,[551]6.0107,[552]6.0089,[553]6.0053,[554]6.0028,[555]5.9995,[556]5.9988,[557]6.0014,[558]5.9979,[559]5.9983,[560]5.9981,[561]5.9982,[562]5.9960,[563]5.9955,[564]6.0005,[565]6.0018,[566]6.0023,[567]5.9998,[568]5.9997,[569]5.9983,[570]6.0010,[571]6.0015,[572]6.0022,[573]6.0020,[574]5.9978,[575]5.9961,[576]5.9961,[577]5.9944,[578]5.9928,[579]5.9929,[580]5.9874,[581]5.9842,[582]5.9833,[583]5.9844,[584]5.9844,[585]5.9770,[586]5.9705,[587]5.9702,[588]5.9741,[589]5.9793,[590]5.9820,[591]5.9836,[592]5.9821,[593]5.9779,[594]5.9785,[595]5.9764,[596]5.9806,[597]5.9778,[598]5.9747,[599]5.9766,[600]5.9756,[601]5.9741,[602]5.9746,[603]5.9764,[604]5.9778,[605]5.9811,[606]5.9828,[607]5.9818,[608]5.9777,[609]5.9784,[610]5.9822,[611]5.9809,[612]5.9831,[613]5.9805,[614]5.9756,[615]5.9689,[616]5.9717,[617]5.9656,[618]5.9604,[619]5.9555,[620]5.9420,[621]5.9354,[622]5.9331,[623]5.9347,[624]5.9349,[625]5.9357,[626]5.9352,[627]5.9382,[628]5.9391,[629]5.9393,[630]5.9425,[631]5.9481,[632]5.9539,[633]5.9534,[634]5.9563,[635]5.9576,[636]5.9551,[637]5.9516,[638]5.9540,[639]5.9499,[640]5.9512,[641]5.9514,[642]5.9572,[643]5.9588,[644]5.9600,[645]5.9585,[646]5.9622,[647]5.9587,[648]5.9593,[649]5.9600,[650]5.9634,[651]5.9677,[652]5.9684,[653]5.9718,[654]5.9656,[655]5.9647,
llama_print_timings:        load time =    1099.52 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =   70467.20 ms / 335360 tokens (    0.21 ms per token,  4759.09 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =   95171.83 ms / 335361 tokens

Final estimate: PPL = 5.9647 +/- 0.03350
