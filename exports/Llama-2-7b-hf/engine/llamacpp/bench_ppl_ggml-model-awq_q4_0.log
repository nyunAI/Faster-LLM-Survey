main: build = 1833 (326b418b)
main: built with cc (Debian 10.2.1-6) 10.2.1 20210110 for x86_64-linux-gnu
main: seed  = 1706107626
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from exports/Llama-2-7b-hf/engine/llamacpp/ggml-model-awq_q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
llm_load_print_meta: general.name     = LLaMA v2
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size       =    0.11 MiB
llm_load_tensors: using CUDA for GPU acceleration
llm_load_tensors: system memory used  =   70.42 MiB
llm_load_tensors: VRAM used           = 3577.55 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
..................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init: VRAM kv self = 256.00 MB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_build_graph: non-view tensors processed: 676/676
llama_new_context_with_model: compute buffer total size = 73.69 MiB
llama_new_context_with_model: VRAM scratch buffer: 70.50 MiB
llama_new_context_with_model: total VRAM used: 3904.05 MiB (model: 3577.55 MiB, context: 326.50 MiB)

system_info: n_threads = 32 / 12 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 1116.97 ms
perplexity: calculating perplexity over 655 chunks, batch_size=512
perplexity: 0.19 seconds per pass - ETA 2.07 minutes
[1]4.3225,[2]4.8068,[3]5.4559,[4]6.0366,[5]6.1693,[6]6.0471,[7]6.2261,[8]6.3088,[9]6.6458,[10]6.8519,[11]7.0632,[12]7.1002,[13]7.0248,[14]7.1072,[15]7.3411,[16]6.9977,[17]6.8809,[18]6.8694,[19]6.5427,[20]6.5381,[21]6.4516,[22]6.2785,[23]6.2475,[24]6.1505,[25]6.1473,[26]5.9910,[27]5.8079,[28]5.7105,[29]5.6283,[30]5.4767,[31]5.4405,[32]5.4655,[33]5.4196,[34]5.4469,[35]5.4625,[36]5.4876,[37]5.4819,[38]5.4796,[39]5.4974,[40]5.5474,[41]5.5662,[42]5.6072,[43]5.5683,[44]5.6207,[45]5.6337,[46]5.6112,[47]5.6330,[48]5.6106,[49]5.6091,[50]5.5729,[51]5.5745,[52]5.5651,[53]5.6106,[54]5.5980,[55]5.5801,[56]5.6116,[57]5.6323,[58]5.6607,[59]5.6840,[60]5.7370,[61]5.7332,[62]5.7969,[63]5.8330,[64]5.8397,[65]5.8874,[66]5.8990,[67]5.9174,[68]5.9402,[69]5.9751,[70]6.0138,[71]6.0393,[72]6.0751,[73]6.1295,[74]6.1410,[75]6.1540,[76]6.1701,[77]6.1842,[78]6.1734,[79]6.1991,[80]6.1981,[81]6.2139,[82]6.2189,[83]6.1695,[84]6.1609,[85]6.1563,[86]6.1386,[87]6.0840,[88]6.0589,[89]6.0423,[90]6.0331,[91]6.0534,[92]6.0523,[93]6.0532,[94]6.0530,[95]6.0808,[96]6.0791,[97]6.0705,[98]6.0651,[99]6.0548,[100]6.0572,[101]6.0804,[102]6.0770,[103]6.0935,[104]6.1033,[105]6.1051,[106]6.1214,[107]6.1244,[108]6.1395,[109]6.1386,[110]6.1341,[111]6.1536,[112]6.1729,[113]6.1737,[114]6.1727,[115]6.1801,[116]6.1687,[117]6.1716,[118]6.1964,[119]6.2161,[120]6.2512,[121]6.2683,[122]6.2907,[123]6.3314,[124]6.3488,[125]6.3402,[126]6.3766,[127]6.4129,[128]6.4379,[129]6.4218,[130]6.4310,[131]6.4250,[132]6.4168,[133]6.4029,[134]6.4108,[135]6.4086,[136]6.3974,[137]6.3920,[138]6.3737,[139]6.3658,[140]6.3636,[141]6.3411,[142]6.3373,[143]6.3091,[144]6.2894,[145]6.2807,[146]6.2715,[147]6.2786,[148]6.2804,[149]6.2747,[150]6.2748,[151]6.2805,[152]6.2733,[153]6.2602,[154]6.2530,[155]6.2601,[156]6.2579,[157]6.2734,[158]6.2773,[159]6.2791,[160]6.2826,[161]6.2952,[162]6.2673,[163]6.2555,[164]6.2303,[165]6.1992,[166]6.1712,[167]6.1327,[168]6.1016,[169]6.0865,[170]6.0749,[171]6.0503,[172]6.0363,[173]6.0201,[174]5.9919,[175]5.9705,[176]5.9566,[177]5.9369,[178]5.9143,[179]5.8984,[180]5.8894,[181]5.8709,[182]5.8525,[183]5.8385,[184]5.8362,[185]5.8277,[186]5.8287,[187]5.8337,[188]5.8329,[189]5.8486,[190]5.8497,[191]5.8679,[192]5.8854,[193]5.9029,[194]5.9167,[195]5.9393,[196]5.9539,[197]5.9736,[198]5.9886,[199]5.9913,[200]5.9954,[201]5.9883,[202]6.0046,[203]6.0136,[204]6.0122,[205]6.0232,[206]6.0267,[207]6.0249,[208]6.0345,[209]6.0372,[210]6.0429,[211]6.0528,[212]6.0583,[213]6.0684,[214]6.0700,[215]6.0722,[216]6.0855,[217]6.1020,[218]6.1159,[219]6.1156,[220]6.1127,[221]6.1046,[222]6.1020,[223]6.0931,[224]6.0836,[225]6.0783,[226]6.0985,[227]6.1033,[228]6.1094,[229]6.1150,[230]6.1109,[231]6.1250,[232]6.1142,[233]6.0989,[234]6.0843,[235]6.0618,[236]6.0557,[237]6.0433,[238]6.0461,[239]6.0322,[240]6.0218,[241]6.0239,[242]6.0259,[243]6.0219,[244]6.0107,[245]6.0063,[246]5.9945,[247]5.9834,[248]5.9762,[249]5.9726,[250]5.9764,[251]5.9679,[252]5.9639,[253]5.9540,[254]5.9484,[255]5.9377,[256]5.9205,[257]5.9087,[258]5.9008,[259]5.9012,[260]5.8928,[261]5.8871,[262]5.8811,[263]5.8756,[264]5.8526,[265]5.8519,[266]5.8487,[267]5.8427,[268]5.8513,[269]5.8507,[270]5.8515,[271]5.8578,[272]5.8611,[273]5.8615,[274]5.8632,[275]5.8703,[276]5.8765,[277]5.8918,[278]5.9005,[279]5.9107,[280]5.9137,[281]5.9246,[282]5.9292,[283]5.9446,[284]5.9544,[285]5.9638,[286]5.9773,[287]5.9764,[288]5.9823,[289]5.9745,[290]5.9585,[291]5.9442,[292]5.9285,[293]5.9141,[294]5.9143,[295]5.9136,[296]5.9180,[297]5.9162,[298]5.9174,[299]5.9135,[300]5.9021,[301]5.9009,[302]5.8933,[303]5.8855,[304]5.8759,[305]5.8714,[306]5.8589,[307]5.8610,[308]5.8623,[309]5.8461,[310]5.8417,[311]5.8358,[312]5.8358,[313]5.8295,[314]5.8262,[315]5.8102,[316]5.8052,[317]5.7901,[318]5.7706,[319]5.7841,[320]5.7973,[321]5.8030,[322]5.7984,[323]5.7930,[324]5.7920,[325]5.8029,[326]5.8034,[327]5.8055,[328]5.8092,[329]5.8151,[330]5.8177,[331]5.8297,[332]5.8257,[333]5.8331,[334]5.8266,[335]5.8202,[336]5.8221,[337]5.8205,[338]5.8202,[339]5.8154,[340]5.8114,[341]5.8183,[342]5.8198,[343]5.8244,[344]5.8240,[345]5.8244,[346]5.8215,[347]5.8239,[348]5.8272,[349]5.8297,[350]5.8279,[351]5.8285,[352]5.8285,[353]5.8220,[354]5.8211,[355]5.8253,[356]5.8281,[357]5.8253,[358]5.8341,[359]5.8366,[360]5.8334,[361]5.8327,[362]5.8400,[363]5.8513,[364]5.8574,[365]5.8622,[366]5.8646,[367]5.8721,[368]5.8688,[369]5.8702,[370]5.8719,[371]5.8670,[372]5.8727,[373]5.8766,[374]5.8751,[375]5.8735,[376]5.8809,[377]5.8773,[378]5.8805,[379]5.8853,[380]5.8775,[381]5.8745,[382]5.8688,[383]5.8668,[384]5.8657,[385]5.8646,[386]5.8652,[387]5.8650,[388]5.8597,[389]5.8552,[390]5.8487,[391]5.8417,[392]5.8368,[393]5.8371,[394]5.8395,[395]5.8374,[396]5.8294,[397]5.8367,[398]5.8408,[399]5.8480,[400]5.8469,[401]5.8489,[402]5.8498,[403]5.8519,[404]5.8576,[405]5.8489,[406]5.8451,[407]5.8447,[408]5.8465,[409]5.8576,[410]5.8686,[411]5.8780,[412]5.8933,[413]5.9043,[414]5.9104,[415]5.9162,[416]5.9234,[417]5.9353,[418]5.9388,[419]5.9439,[420]5.9518,[421]5.9624,[422]5.9662,[423]5.9718,[424]5.9810,[425]5.9895,[426]5.9961,[427]5.9997,[428]6.0074,[429]6.0124,[430]6.0196,[431]6.0339,[432]6.0375,[433]6.0359,[434]6.0320,[435]6.0335,[436]6.0360,[437]6.0452,[438]6.0527,[439]6.0490,[440]6.0465,[441]6.0419,[442]6.0408,[443]6.0423,[444]6.0431,[445]6.0414,[446]6.0433,[447]6.0455,[448]6.0494,[449]6.0470,[450]6.0473,[451]6.0436,[452]6.0296,[453]6.0200,[454]6.0140,[455]6.0148,[456]6.0195,[457]6.0213,[458]6.0195,[459]6.0193,[460]6.0275,[461]6.0239,[462]6.0224,[463]6.0262,[464]6.0254,[465]6.0235,[466]6.0164,[467]6.0180,[468]6.0185,[469]6.0203,[470]6.0210,[471]6.0172,[472]6.0210,[473]6.0151,[474]6.0166,[475]6.0101,[476]6.0120,[477]6.0049,[478]6.0040,[479]6.0098,[480]6.0150,[481]6.0176,[482]6.0135,[483]6.0095,[484]6.0112,[485]6.0093,[486]6.0047,[487]6.0040,[488]6.0018,[489]5.9976,[490]5.9957,[491]5.9930,[492]5.9876,[493]5.9845,[494]5.9825,[495]5.9804,[496]5.9770,[497]5.9721,[498]5.9705,[499]5.9662,[500]5.9576,[501]5.9520,[502]5.9520,[503]5.9504,[504]5.9421,[505]5.9439,[506]5.9445,[507]5.9394,[508]5.9350,[509]5.9352,[510]5.9373,[511]5.9423,[512]5.9458,[513]5.9474,[514]5.9533,[515]5.9483,[516]5.9478,[517]5.9480,[518]5.9474,[519]5.9497,[520]5.9522,[521]5.9534,[522]5.9560,[523]5.9564,[524]5.9616,[525]5.9650,[526]5.9665,[527]5.9679,[528]5.9635,[529]5.9636,[530]5.9590,[531]5.9568,[532]5.9618,[533]5.9636,[534]5.9627,[535]5.9657,[536]5.9602,[537]5.9587,[538]5.9631,[539]5.9645,[540]5.9672,[541]5.9678,[542]5.9684,[543]5.9705,[544]5.9714,[545]5.9697,[546]5.9706,[547]5.9666,[548]5.9615,[549]5.9609,[550]5.9586,[551]5.9560,[552]5.9541,[553]5.9504,[554]5.9478,[555]5.9448,[556]5.9443,[557]5.9468,[558]5.9434,[559]5.9439,[560]5.9439,[561]5.9443,[562]5.9417,[563]5.9412,[564]5.9456,[565]5.9465,[566]5.9470,[567]5.9445,[568]5.9446,[569]5.9432,[570]5.9462,[571]5.9468,[572]5.9476,[573]5.9473,[574]5.9436,[575]5.9422,[576]5.9422,[577]5.9406,[578]5.9391,[579]5.9391,[580]5.9333,[581]5.9304,[582]5.9298,[583]5.9310,[584]5.9312,[585]5.9240,[586]5.9176,[587]5.9175,[588]5.9215,[589]5.9268,[590]5.9294,[591]5.9313,[592]5.9300,[593]5.9260,[594]5.9264,[595]5.9245,[596]5.9284,[597]5.9259,[598]5.9229,[599]5.9248,[600]5.9238,[601]5.9224,[602]5.9227,[603]5.9244,[604]5.9258,[605]5.9290,[606]5.9306,[607]5.9295,[608]5.9255,[609]5.9264,[610]5.9301,[611]5.9288,[612]5.9313,[613]5.9287,[614]5.9240,[615]5.9176,[616]5.9202,[617]5.9141,[618]5.9091,[619]5.9041,[620]5.8908,[621]5.8842,[622]5.8820,[623]5.8840,[624]5.8841,[625]5.8851,[626]5.8846,[627]5.8875,[628]5.8882,[629]5.8886,[630]5.8919,[631]5.8973,[632]5.9031,[633]5.9027,[634]5.9056,[635]5.9067,[636]5.9041,[637]5.9003,[638]5.9027,[639]5.8989,[640]5.9004,[641]5.9007,[642]5.9065,[643]5.9082,[644]5.9094,[645]5.9080,[646]5.9118,[647]5.9081,[648]5.9088,[649]5.9096,[650]5.9131,[651]5.9172,[652]5.9180,[653]5.9215,[654]5.9153,[655]5.9143,
llama_print_timings:        load time =    1096.21 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =   70423.35 ms / 335360 tokens (    0.21 ms per token,  4762.06 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =   94959.60 ms / 335361 tokens

Final estimate: PPL = 5.9143 +/- 0.03315
