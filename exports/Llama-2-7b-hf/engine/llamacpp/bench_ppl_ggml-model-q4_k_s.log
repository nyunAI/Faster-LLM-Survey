main: build = 1833 (326b418b)
main: built with cc (Debian 10.2.1-6) 10.2.1 20210110 for x86_64-linux-gnu
main: seed  = 1706107407
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from exports/Llama-2-7b-hf/engine/llamacpp/ggml-model-q4_k_s.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 14
llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  20:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_K:  217 tensors
llama_model_loader: - type q5_K:    8 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 4096
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 4096
llm_load_print_meta: n_embd_v_gqa     = 4096
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 11008
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 4096
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q4_K - Small
llm_load_print_meta: model params     = 6.74 B
llm_load_print_meta: model size       = 3.59 GiB (4.58 BPW) 
llm_load_print_meta: general.name     = LLaMA v2
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size       =    0.11 MiB
llm_load_tensors: using CUDA for GPU acceleration
llm_load_tensors: system memory used  =   70.42 MiB
llm_load_tensors: VRAM used           = 3607.05 MiB
llm_load_tensors: offloading 32 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 33/33 layers to GPU
..................................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init: VRAM kv self = 256.00 MB
llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB
llama_build_graph: non-view tensors processed: 676/676
llama_new_context_with_model: compute buffer total size = 73.69 MiB
llama_new_context_with_model: VRAM scratch buffer: 70.50 MiB
llama_new_context_with_model: total VRAM used: 3933.55 MiB (model: 3607.05 MiB, context: 326.50 MiB)

system_info: n_threads = 32 / 12 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | 
perplexity: tokenizing the input ..
perplexity: tokenization took 1064.39 ms
perplexity: calculating perplexity over 655 chunks, batch_size=512
perplexity: 0.19 seconds per pass - ETA 2.08 minutes
[1]4.2615,[2]4.8100,[3]5.4611,[4]6.0282,[5]6.1184,[6]6.0363,[7]6.2226,[8]6.3215,[9]6.6596,[10]6.8530,[11]7.0799,[12]7.1372,[13]7.0577,[14]7.1198,[15]7.3540,[16]7.0127,[17]6.8928,[18]6.8741,[19]6.5438,[20]6.5286,[21]6.4454,[22]6.2740,[23]6.2364,[24]6.1432,[25]6.1353,[26]5.9799,[27]5.7941,[28]5.6930,[29]5.6121,[30]5.4577,[31]5.4191,[32]5.4374,[33]5.3944,[34]5.4237,[35]5.4398,[36]5.4628,[37]5.4565,[38]5.4548,[39]5.4717,[40]5.5252,[41]5.5455,[42]5.5827,[43]5.5455,[44]5.5993,[45]5.6081,[46]5.5831,[47]5.6083,[48]5.5876,[49]5.5861,[50]5.5507,[51]5.5499,[52]5.5406,[53]5.5859,[54]5.5731,[55]5.5589,[56]5.5904,[57]5.6115,[58]5.6389,[59]5.6621,[60]5.7135,[61]5.7082,[62]5.7707,[63]5.8072,[64]5.8132,[65]5.8583,[66]5.8694,[67]5.8898,[68]5.9101,[69]5.9445,[70]5.9804,[71]6.0098,[72]6.0445,[73]6.0991,[74]6.1097,[75]6.1194,[76]6.1360,[77]6.1494,[78]6.1401,[79]6.1674,[80]6.1655,[81]6.1821,[82]6.1882,[83]6.1392,[84]6.1294,[85]6.1249,[86]6.1081,[87]6.0553,[88]6.0304,[89]6.0153,[90]6.0054,[91]6.0283,[92]6.0263,[93]6.0258,[94]6.0253,[95]6.0543,[96]6.0533,[97]6.0463,[98]6.0416,[99]6.0297,[100]6.0314,[101]6.0539,[102]6.0493,[103]6.0675,[104]6.0756,[105]6.0768,[106]6.0937,[107]6.0955,[108]6.1093,[109]6.1078,[110]6.1044,[111]6.1242,[112]6.1459,[113]6.1488,[114]6.1493,[115]6.1569,[116]6.1458,[117]6.1506,[118]6.1761,[119]6.1970,[120]6.2316,[121]6.2486,[122]6.2700,[123]6.3102,[124]6.3276,[125]6.3191,[126]6.3543,[127]6.3895,[128]6.4148,[129]6.3979,[130]6.4066,[131]6.4005,[132]6.3906,[133]6.3761,[134]6.3839,[135]6.3806,[136]6.3692,[137]6.3622,[138]6.3443,[139]6.3374,[140]6.3341,[141]6.3098,[142]6.3044,[143]6.2775,[144]6.2584,[145]6.2501,[146]6.2402,[147]6.2468,[148]6.2494,[149]6.2428,[150]6.2436,[151]6.2497,[152]6.2415,[153]6.2276,[154]6.2204,[155]6.2272,[156]6.2250,[157]6.2392,[158]6.2418,[159]6.2437,[160]6.2462,[161]6.2594,[162]6.2312,[163]6.2185,[164]6.1938,[165]6.1629,[166]6.1345,[167]6.0965,[168]6.0653,[169]6.0512,[170]6.0410,[171]6.0162,[172]6.0016,[173]5.9864,[174]5.9579,[175]5.9363,[176]5.9227,[177]5.9029,[178]5.8814,[179]5.8649,[180]5.8559,[181]5.8377,[182]5.8197,[183]5.8056,[184]5.8032,[185]5.7962,[186]5.7981,[187]5.8032,[188]5.8016,[189]5.8171,[190]5.8169,[191]5.8348,[192]5.8526,[193]5.8706,[194]5.8844,[195]5.9066,[196]5.9210,[197]5.9407,[198]5.9551,[199]5.9580,[200]5.9617,[201]5.9544,[202]5.9716,[203]5.9804,[204]5.9782,[205]5.9894,[206]5.9934,[207]5.9909,[208]6.0003,[209]6.0036,[210]6.0093,[211]6.0198,[212]6.0257,[213]6.0349,[214]6.0365,[215]6.0391,[216]6.0531,[217]6.0688,[218]6.0826,[219]6.0822,[220]6.0791,[221]6.0715,[222]6.0695,[223]6.0608,[224]6.0519,[225]6.0473,[226]6.0673,[227]6.0714,[228]6.0773,[229]6.0822,[230]6.0775,[231]6.0925,[232]6.0805,[233]6.0644,[234]6.0488,[235]6.0263,[236]6.0199,[237]6.0082,[238]6.0102,[239]5.9963,[240]5.9851,[241]5.9874,[242]5.9893,[243]5.9850,[244]5.9736,[245]5.9690,[246]5.9576,[247]5.9464,[248]5.9385,[249]5.9343,[250]5.9380,[251]5.9293,[252]5.9245,[253]5.9146,[254]5.9087,[255]5.8972,[256]5.8797,[257]5.8682,[258]5.8595,[259]5.8594,[260]5.8506,[261]5.8446,[262]5.8390,[263]5.8331,[264]5.8097,[265]5.8088,[266]5.8059,[267]5.7995,[268]5.8081,[269]5.8076,[270]5.8083,[271]5.8145,[272]5.8176,[273]5.8174,[274]5.8187,[275]5.8255,[276]5.8314,[277]5.8464,[278]5.8557,[279]5.8653,[280]5.8681,[281]5.8787,[282]5.8837,[283]5.8985,[284]5.9079,[285]5.9170,[286]5.9297,[287]5.9292,[288]5.9360,[289]5.9285,[290]5.9121,[291]5.8980,[292]5.8821,[293]5.8683,[294]5.8681,[295]5.8669,[296]5.8716,[297]5.8696,[298]5.8714,[299]5.8675,[300]5.8560,[301]5.8550,[302]5.8475,[303]5.8393,[304]5.8303,[305]5.8260,[306]5.8136,[307]5.8152,[308]5.8162,[309]5.8003,[310]5.7960,[311]5.7901,[312]5.7902,[313]5.7844,[314]5.7813,[315]5.7655,[316]5.7606,[317]5.7460,[318]5.7260,[319]5.7394,[320]5.7522,[321]5.7572,[322]5.7528,[323]5.7471,[324]5.7464,[325]5.7572,[326]5.7579,[327]5.7597,[328]5.7639,[329]5.7696,[330]5.7723,[331]5.7843,[332]5.7806,[333]5.7878,[334]5.7812,[335]5.7748,[336]5.7764,[337]5.7748,[338]5.7748,[339]5.7704,[340]5.7664,[341]5.7737,[342]5.7752,[343]5.7797,[344]5.7792,[345]5.7792,[346]5.7767,[347]5.7793,[348]5.7828,[349]5.7857,[350]5.7835,[351]5.7840,[352]5.7841,[353]5.7780,[354]5.7774,[355]5.7814,[356]5.7842,[357]5.7815,[358]5.7900,[359]5.7928,[360]5.7900,[361]5.7894,[362]5.7969,[363]5.8082,[364]5.8145,[365]5.8197,[366]5.8223,[367]5.8304,[368]5.8276,[369]5.8293,[370]5.8310,[371]5.8260,[372]5.8320,[373]5.8364,[374]5.8352,[375]5.8340,[376]5.8416,[377]5.8383,[378]5.8414,[379]5.8461,[380]5.8387,[381]5.8358,[382]5.8307,[383]5.8289,[384]5.8276,[385]5.8260,[386]5.8265,[387]5.8265,[388]5.8214,[389]5.8168,[390]5.8107,[391]5.8038,[392]5.7993,[393]5.7994,[394]5.8021,[395]5.8002,[396]5.7927,[397]5.7996,[398]5.8035,[399]5.8107,[400]5.8094,[401]5.8112,[402]5.8120,[403]5.8143,[404]5.8206,[405]5.8119,[406]5.8080,[407]5.8075,[408]5.8099,[409]5.8211,[410]5.8320,[411]5.8415,[412]5.8567,[413]5.8679,[414]5.8741,[415]5.8800,[416]5.8874,[417]5.8995,[418]5.9035,[419]5.9085,[420]5.9168,[421]5.9273,[422]5.9311,[423]5.9368,[424]5.9465,[425]5.9550,[426]5.9616,[427]5.9652,[428]5.9725,[429]5.9776,[430]5.9845,[431]5.9987,[432]6.0020,[433]6.0005,[434]5.9965,[435]5.9978,[436]6.0006,[437]6.0099,[438]6.0170,[439]6.0132,[440]6.0111,[441]6.0065,[442]6.0052,[443]6.0067,[444]6.0073,[445]6.0056,[446]6.0076,[447]6.0098,[448]6.0136,[449]6.0110,[450]6.0111,[451]6.0073,[452]5.9936,[453]5.9842,[454]5.9781,[455]5.9784,[456]5.9827,[457]5.9845,[458]5.9826,[459]5.9824,[460]5.9904,[461]5.9868,[462]5.9855,[463]5.9893,[464]5.9883,[465]5.9862,[466]5.9791,[467]5.9806,[468]5.9810,[469]5.9830,[470]5.9836,[471]5.9801,[472]5.9840,[473]5.9783,[474]5.9803,[475]5.9741,[476]5.9761,[477]5.9692,[478]5.9685,[479]5.9744,[480]5.9794,[481]5.9816,[482]5.9780,[483]5.9743,[484]5.9758,[485]5.9746,[486]5.9701,[487]5.9695,[488]5.9674,[489]5.9631,[490]5.9615,[491]5.9589,[492]5.9536,[493]5.9503,[494]5.9483,[495]5.9465,[496]5.9427,[497]5.9380,[498]5.9366,[499]5.9321,[500]5.9235,[501]5.9181,[502]5.9182,[503]5.9165,[504]5.9079,[505]5.9098,[506]5.9102,[507]5.9049,[508]5.9004,[509]5.9002,[510]5.9023,[511]5.9069,[512]5.9105,[513]5.9121,[514]5.9178,[515]5.9128,[516]5.9123,[517]5.9125,[518]5.9122,[519]5.9146,[520]5.9169,[521]5.9179,[522]5.9202,[523]5.9207,[524]5.9260,[525]5.9290,[526]5.9307,[527]5.9322,[528]5.9274,[529]5.9278,[530]5.9231,[531]5.9210,[532]5.9263,[533]5.9284,[534]5.9273,[535]5.9302,[536]5.9249,[537]5.9234,[538]5.9280,[539]5.9292,[540]5.9310,[541]5.9322,[542]5.9328,[543]5.9352,[544]5.9363,[545]5.9351,[546]5.9360,[547]5.9321,[548]5.9270,[549]5.9266,[550]5.9240,[551]5.9213,[552]5.9198,[553]5.9161,[554]5.9138,[555]5.9105,[556]5.9100,[557]5.9125,[558]5.9091,[559]5.9097,[560]5.9096,[561]5.9100,[562]5.9073,[563]5.9070,[564]5.9116,[565]5.9129,[566]5.9136,[567]5.9106,[568]5.9106,[569]5.9092,[570]5.9121,[571]5.9129,[572]5.9135,[573]5.9134,[574]5.9097,[575]5.9083,[576]5.9080,[577]5.9062,[578]5.9046,[579]5.9048,[580]5.8993,[581]5.8963,[582]5.8955,[583]5.8967,[584]5.8969,[585]5.8894,[586]5.8831,[587]5.8828,[588]5.8868,[589]5.8921,[590]5.8946,[591]5.8965,[592]5.8951,[593]5.8912,[594]5.8918,[595]5.8897,[596]5.8935,[597]5.8911,[598]5.8880,[599]5.8899,[600]5.8890,[601]5.8874,[602]5.8875,[603]5.8891,[604]5.8909,[605]5.8943,[606]5.8963,[607]5.8955,[608]5.8914,[609]5.8921,[610]5.8961,[611]5.8944,[612]5.8966,[613]5.8939,[614]5.8891,[615]5.8825,[616]5.8853,[617]5.8792,[618]5.8741,[619]5.8690,[620]5.8558,[621]5.8494,[622]5.8472,[623]5.8489,[624]5.8492,[625]5.8502,[626]5.8497,[627]5.8525,[628]5.8532,[629]5.8536,[630]5.8569,[631]5.8623,[632]5.8681,[633]5.8676,[634]5.8709,[635]5.8723,[636]5.8696,[637]5.8659,[638]5.8684,[639]5.8646,[640]5.8659,[641]5.8662,[642]5.8718,[643]5.8733,[644]5.8746,[645]5.8731,[646]5.8770,[647]5.8732,[648]5.8741,[649]5.8746,[650]5.8779,[651]5.8822,[652]5.8831,[653]5.8865,[654]5.8803,[655]5.8791,
llama_print_timings:        load time =    1090.55 ms
llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings: prompt eval time =   71849.01 ms / 335360 tokens (    0.21 ms per token,  4667.57 tokens per second)
llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
llama_print_timings:       total time =   96869.73 ms / 335361 tokens

Final estimate: PPL = 5.8791 +/- 0.03277
