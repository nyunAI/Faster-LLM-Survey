You passed `quantization_config` to `from_pretrained` but the model you're loading already has a `quantization_config` attribute and has already quantized weights. However, loading attributes (e.g. ['use_cuda_fp16', 'use_exllama', 'max_input_length', 'exllama_config', 'disable_exllama']) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.
BASE: /home/shwu/LLM-Efficiency-Survey
MODEL: /home/shwu/LLM-Efficiency-Survey/model
MODEL_PATH: /home/shwu/LLM-Efficiency-Survey/model/Llama-2-7b-hf
METHOD_EXPORTS: /home/shwu/LLM-Efficiency-Survey/exports/Llama-2-7b-hf/quant/hf
Quant: gptq_8
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.82s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.23s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.32s/it]
Memory footprint: 6.7306671142578125 GiB
Output: What is the meaning of life? What is the meaning of existence? What is the meaning of human life? These questions are very important to us, but we don’t know how to answer them.
The meaning of life is something that we don’t know. It’s something that we can’t explain. It’s something that we can’t understand. It’s something that we can’t see. It’s something that we can’t hear. It’s something that we can’t feel. It’s something that we can’t taste. It’s something that we can’t smell. It’
----------------------------------- Stats -----------------------------------
Generated 128 tokens in 7.2449281215667725 seconds (17.667532079299498 tokens/s)
--------------------------------------------------------------------------------


